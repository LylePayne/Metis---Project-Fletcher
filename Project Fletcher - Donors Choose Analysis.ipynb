{
 "metadata": {
  "name": "",
  "signature": "sha256:cb0d5437d85499f3faccdd2f1c22d8d1419c8e3cb3c03a601b849968dd6b993c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Project Fletcher - Analysis of Text Data:\n",
      "###Project Source - Donors Choose Essay Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Donors Choose is a funding site that allows teachers to post classroom projects and request funding for resources for their classrooms. We sought to understand what the most prevelant need groups and the most apparent needs were by utilizing NLTK to parse and analyse the essay files.\n",
      "\n",
      "Data sourced from here: http://data.donorschoose.org/open-data/project-essay-data/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.tokenize import sent_tokenize\n",
      "from nltk.tokenize import TreebankWordTokenizer\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.tokenize import wordpunct_tokenize\n",
      "from nltk.tag import pos_tag\n",
      "from nltk.chunk import ne_chunk\n",
      "from nltk.corpus import treebank_chunk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.util import ngrams\n",
      "from textblob import TextBlob\n",
      "\n",
      "from collections import Counter\n",
      "from collections import defaultdict\n",
      "from operator import itemgetter\n",
      "\n",
      "import pymongo\n",
      "from pymongo import MongoClient\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.cluster import MiniBatchKMeans\n",
      "from sklearn.metrics import pairwise_distances\n",
      "from sklearn.preprocessing import normalize\n",
      "\n",
      "import random\n",
      "import pickle\n",
      "import traceback"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Creating the Donors Choose Database within MongoDB"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mongo_client = MongoClient()\n",
      "db = mongo_client['DonorsChooseDB']\n",
      "#creating the donor's choose database\n",
      "DC_collection = db['Essays']\n",
      "#defining the collection within the database"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#need to increase the field size limit to be able to import the data into MongoDB\n",
      "import csv\n",
      "import sys\n",
      "csv.field_size_limit(sys.maxsize)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "131072"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##Pulling in the Donor's Choose Essay Data and storing it in MongoDB Collection\n",
      "\n",
      "with open('opendata_essays.csv') as csvfile:\n",
      "    records = csv.DictReader(csvfile)\n",
      "    for i, record in enumerate(records):\n",
      "        try:\n",
      "            if i% 25000 == 0:\n",
      "                ##Adding sys error print to be able to see progress\n",
      "                print >> sys.stderr, \"Processing %i\" %i\n",
      "            DC_collection.save(record)\n",
      "        except:\n",
      "            print traceback.format_exc()\n",
      "            ##Prints out entry where the document is too large for Mongo\n",
      "            for key, val in record.iteritems():\n",
      "                try:\n",
      "                    ##Prints first hundred characters and the last hundred characters of the field\n",
      "                    print key, \":\", val[:100], val[-100:]\n",
      "                    print\n",
      "                except:\n",
      "                    continue"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Processing 0\n",
        "Processing 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 50000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 75000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 100000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 125000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 150000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 175000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 200000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 225000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 250000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 275000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 300000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 325000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 350000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 375000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 400000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 425000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 450000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 475000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 500000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 525000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 550000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 575000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 600000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 625000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 650000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 675000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 700000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 725000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 750000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Traceback (most recent call last):\n",
        "  File \"<ipython-input-6-c2d8ee115e74>\", line 7, in <module>\n",
        "    DC_collection.save(record)\n",
        "  File \"/usr/local/lib/python2.7/site-packages/pymongo/collection.py\", line 285, in save\n",
        "    return self.insert(to_save, manipulate, safe, check_keys, **kwargs)\n",
        "  File \"/usr/local/lib/python2.7/site-packages/pymongo/collection.py\", line 409, in insert\n",
        "    gen(), check_keys, self.uuid_subtype, client)\n",
        "DocumentTooLarge: BSON document too large (16859235 bytes) - the connected server supports BSON document sizes up to 16777216 bytes.\n",
        "\n",
        "essay : \"Every day, high school students come patiently into my classroom, and we struggle to cover decades  fference for our students as they work to obtain their education and regain their chances in life! \"\n",
        "\n",
        "title : \"Super Simple Beautiful Whiteboard\" \"Super Simple Beautiful Whiteboard\"\n",
        "\n",
        "need_statement : \"My students need slick, high-quality basic whiteboards to replace our peeling chalkboards from the  dents need slick, high-quality basic whiteboards to replace our peeling chalkboards from the 1920s!\"\n",
        "\n",
        "impact_letter : \"Mrs. (Jalali) NelsonMMrs. (Jalali) NelsonrMrs. (Jalali) NelsonsMrs. (Jalali) Nelson.Mrs. (Jalali) N (Jalali) NelsonlMrs. (Jalali) NelsonsMrs. (Jalali) NelsonoMrs. (Jalali) NelsonnMrs. (Jalali) Nelson\"\n",
        "\n",
        "thankyou_note :  \n",
        "\n",
        "paragraph4 : \"Please support our class by donating to our whiteboard fund today.  Something as simple as a \"\"clea fference for our students as they work to obtain their education and regain their chances in life! \"\n",
        "\n",
        "_projectid : \"6acc067216b85e865b8eee9c96440efc\" \"6acc067216b85e865b8eee9c96440efc\"\n",
        "\n",
        "paragraph1 : \"Every day, high school students come patiently into my classroom, and we struggle to cover decades  ative schools for a major urban district, serving many of the most high-need students in the city. \"\n",
        "\n",
        "paragraph2 : \"The range of needs and learning styles in an alternative school setting is vast, ranging from being to struggle with the limited resources of the building that don't even meet basic standards today. \"\n",
        "\n",
        "paragraph3 : \"A clean, brand-new whiteboard would show my students that they deserve to learn just as much as stu out their education, instead of leaving them to feel frustrated by our school's lack of resources. \"\n",
        "\n",
        "short_description : \"A clean, brand-new whiteboard would show my students that they deserve to learn just as much as stu  the difference for our students as they work to obtain their education and regain their chances...\"\n",
        "\n",
        "_id : _teacher_acctid : \"87d64a4161a2d3abce1db01bf162c1fc\" \"87d64a4161a2d3abce1db01bf162c1fc\"\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Create a function to create datasets from a single field"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_text_mongo(collection_data, field_name):\n",
      "    text = [i[field_name] for i in collection_data.find({}, {field_name:1, '_id':0})]\n",
      "    return text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Create a dataset of need statements"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "need_text = get_text_mongo(DC_collection, 'need_statement')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create a sample so we can run faster analysis and understand initial trends"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "small_need_text = random.sample(need_text, 1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##pickle file to have easy access later\n",
      "with open(\"need_text.pkl\", \"w\") as picklefile:\n",
      "    pickle.dump(need_text, picklefile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Create a dataset of title text information"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title_text = get_text_mongo(DC_collection, 'title')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"title_text.pkl\", \"w\") as picklefile:\n",
      "    pickle.dump(title_text, picklefile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Create a dataset of first paragraph text information"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "first_paragraph = get_text_mongo(DC_collection, 'paragraph1')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"first_paragraph.pkl\", \"w\") as picklefile:\n",
      "    pickle.dump(first_paragraph, picklefile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"first_paragraph.pkl\", \"r\") as picklefile:\n",
      "    first_paragraph = pickle.load(picklefile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: 'first_paragraph.pkl'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-16-682bfc514691>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first_paragraph.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpicklefile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mfirst_paragraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpicklefile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'first_paragraph.pkl'"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Looking at the most common phrases in the title to try and understand what teachers ask for"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.util import ngrams\n",
      "from nltk.tokenize import sent_tokenize, word_tokenize\n",
      "\n",
      "from collections import Counter\n",
      "from operator import itemgetter\n",
      "\n",
      "stop = stopwords.words(\"english\")\n",
      "stop += ['.', ',', '(', ')', \" ' \", \"The\", \"A\", \"n\", \"In\", \"My\", \"We\", \"Need\", \"need\", \"I\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counter = Counter()\n",
      "\n",
      "for doc in title_text:\n",
      "    words = TextBlob(doc).words\n",
      "    words = [w for w in words if w not in stop]\n",
      "    bigrams = ngrams(words, 2)\n",
      "    for gram in bigrams:\n",
      "        counter[gram] += 1\n",
      "\n",
      "counter.most_common(100)\n",
      "#First only look at the top 100 bi-grams in the title text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "[((u'Help', u'Us'), 27631),\n",
        " ((u'Let', u\"'s\"), 24211),\n",
        " ((u'We', u'Need'), 20749),\n",
        " ((u'It', u\"'s\"), 12603),\n",
        " ((u'21st', u'Century'), 12159),\n",
        " ((u'Listening', u'Center'), 10412),\n",
        " ((u'Classroom', u'Library'), 10364),\n",
        " ((u'Part', u'2'), 10162),\n",
        " ((u'My', u'Students'), 10033),\n",
        " ((u'First', u'Grade'), 8450),\n",
        " ((u'In', u'The'), 7693),\n",
        " ((u'Special', u'Needs'), 7365),\n",
        " ((u'Our', u'Classroom'), 7330),\n",
        " ((u'Students', u'Need'), 7127),\n",
        " ((u'Help', u'My'), 7075),\n",
        " ((u'To', u'Read'), 6853),\n",
        " ((u'Ca', u\"n't\"), 6772),\n",
        " ((u\"'s\", u'Get'), 6689),\n",
        " ((u'First', u'Graders'), 6654),\n",
        " ((u'Can', u'You'), 6599),\n",
        " ((u'The', u'World'), 6149),\n",
        " ((u'We', u'Love'), 6121),\n",
        " ((u'I', u'Can'), 6009),\n",
        " ((u'We', u'Want'), 5979),\n",
        " ((u'Do', u\"n't\"), 5888),\n",
        " ((u'Learning', u'Through'), 5712),\n",
        " ((u'Document', u'Camera'), 5620),\n",
        " ((u'We', u\"'re\"), 5561),\n",
        " ((u'Through', u'Technology'), 5362),\n",
        " ((u'Please', u'Help'), 5290),\n",
        " ((u'You', u'Hear'), 5061),\n",
        " ((u'Special', u'Education'), 5022),\n",
        " ((u'Common', u'Core'), 4978),\n",
        " ((u'Help', u'Me'), 4860),\n",
        " ((u'For', u'The'), 4832),\n",
        " ((u'Love', u'Reading'), 4775),\n",
        " ((u'Part', u'1'), 4706),\n",
        " ((u'Social', u'Studies'), 4646),\n",
        " ((u'Oh', u'My'), 4621),\n",
        " ((u'All', u'About'), 4582),\n",
        " ((u'Middle', u'School'), 4525),\n",
        " ((u'What', u\"'s\"), 4522),\n",
        " ((u'We', u'Are'), 4507),\n",
        " ((u'Help', u'Our'), 4459),\n",
        " ((u'Do', u'You'), 4446),\n",
        " ((u'Readers', u'Need'), 4404),\n",
        " ((u'Want', u'To'), 4356),\n",
        " ((u'We', u'Can'), 4349),\n",
        " ((u'For', u'Our'), 4330),\n",
        " ((u'In', u'Need'), 4297),\n",
        " ((u'Books', u'Books'), 4275),\n",
        " ((u'To', u'The'), 4210),\n",
        " ((u'High', u'School'), 4028),\n",
        " ((u'With', u'Technology'), 3955),\n",
        " ((u'Need', u'Books'), 3864),\n",
        " ((u'Books', u'For'), 3853),\n",
        " ((u'The', u'Classroom'), 3796),\n",
        " ((u'Hands', u'On'), 3765),\n",
        " ((u'For', u'My'), 3763),\n",
        " ((u'Our', u'Way'), 3719),\n",
        " ((u'Need', u'A'), 3597),\n",
        " ((u'A', u'Place'), 3591),\n",
        " ((u'Lights', u'Camera'), 3564),\n",
        " ((u'About', u'It'), 3550),\n",
        " ((u'To', u'Learn'), 3550),\n",
        " ((u'Second', u'Grade'), 3410),\n",
        " ((u'Making', u'Math'), 3396),\n",
        " ((u'With', u'A'), 3365),\n",
        " ((u'Us', u'Learn'), 3251),\n",
        " ((u'LCD', u'Projector'), 3210),\n",
        " ((u'Part', u'II'), 3193),\n",
        " ((u'What', u'I'), 3188),\n",
        " ((u'A', u'New'), 3160),\n",
        " ((u'On', u'The'), 3156),\n",
        " ((u'Help', u'Students'), 3153),\n",
        " ((u'To', u'Be'), 3148),\n",
        " ((u'Learning', u'With'), 3090),\n",
        " ((u'Listen', u'Up'), 3080),\n",
        " ((u'Reading', u'Writing'), 3074),\n",
        " ((u'Graders', u'Need'), 3065),\n",
        " ((u'Us', u'Get'), 3059),\n",
        " ((u'I', u\"'m\"), 3054),\n",
        " ((u'Read', u'All'), 3035),\n",
        " ((u'Help', u'us'), 2993),\n",
        " ((u'Ready', u'Set'), 2985),\n",
        " ((u'A', u'Picture'), 2947),\n",
        " ((u'Hear', u'Me'), 2943),\n",
        " ((u'Our', u'World'), 2943),\n",
        " ((u'Learning', u'Fun'), 2935),\n",
        " ((u'Art', u'Supplies'), 2895),\n",
        " ((u'Math', u'Science'), 2857),\n",
        " ((u'More', u'Books'), 2832),\n",
        " ((u'Come', u'Alive'), 2817),\n",
        " ((u'School', u'Supplies'), 2811),\n",
        " ((u'Math', u'Manipulatives'), 2770),\n",
        " ((u'Struggling', u'Readers'), 2744),\n",
        " ((u'Financial', u'Literacy'), 2719),\n",
        " ((u'Can', u'Read'), 2711),\n",
        " ((u'Second', u'Graders'), 2704),\n",
        " ((u'My', u'Classroom'), 2660)]"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because the Title Text is often trying to express overall urgency of the need, it's not the most helpful in terms of what people are asking for or who they are targeting with their request, so we decided to continue to explore more parts of the dataset. There were some interesting bigrams that had a high level of occurance such as \"Listening Center\", \"Classroom Library\" and \"Special Needs\"."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Create a function to look at the most common words in any corpus, so we can evaluate multiple fields in the forms and understand which sections hold the most meaning"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def common_words_function(plainText, stopwords, n_gram_length):\n",
      "    counter = Counter()\n",
      "\n",
      "    for doc in plainText:\n",
      "        words = TextBlob(doc).words\n",
      "        words = [w for w in words if w not in stopwords]\n",
      "        ##filters out words included in stopwords (e.g. the or with)\n",
      "        n_grams = ngrams(words, n_gram_length)\n",
      "        ##creates ngrams based on the n-gram-length input\n",
      "        for gram in n_grams:\n",
      "            counter[gram] += 1\n",
      "\n",
      "    return counter.most_common(100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Look at need statement short section most common phrases, based on sample"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "common_words_function(small_need_text, stop)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "[((u'students', u'need'), 8775),\n",
        " ((u'My', u'students'), 8750),\n",
        " ((u'The', u'cost'), 718),\n",
        " ((u'including', u'shipping'), 687),\n",
        " ((u'href', u'http'), 495),\n",
        " ((u'www.donorschoose.org/html/fulfillment.htm', u'onclick'), 495),\n",
        " ((u\"'fulfillwindow\", u'return'), 495),\n",
        " ((u'new', u'href'), 495),\n",
        " ((u'www.donorschoose.org/html/fulfillment.htm', u'300'), 495),\n",
        " ((u'http', u'www.donorschoose.org/html/fulfillment.htm'), 495),\n",
        " ((u'return', u'false'), 495),\n",
        " ((u'target', u'new'), 495),\n",
        " ((u'false', u'fulfillment'), 495),\n",
        " ((u'800', u\"'fulfillwindow\"), 495),\n",
        " ((u'onclick', u'g_openWindow'), 495),\n",
        " ((u'300', u'800'), 495),\n",
        " ((u\"'http\", u'www.donorschoose.org/html/fulfillment.htm'), 495),\n",
        " ((u'g_openWindow', u\"'http\"), 495),\n",
        " ((u'shipping', u'target'), 467),\n",
        " ((u'project', u'needs'), 376),\n",
        " ((u'My', u'project'), 376),\n",
        " ((u'need', u'2'), 312),\n",
        " ((u'price', u'including'), 226),\n",
        " ((u'shipping', u'fulfillment'), 221),\n",
        " ((u'need', u'iPad'), 212),\n",
        " ((u'listening', u'center'), 193),\n",
        " ((u'need', u'4'), 180),\n",
        " ((u'need', u'30'), 174),\n",
        " ((u'need', u'new'), 168),\n",
        " ((u'document', u'camera'), 166),\n",
        " ((u'need', u'3'), 158),\n",
        " ((u'classroom', u'library'), 153),\n",
        " ((u'dry', u'erase'), 149),\n",
        " ((u'need', u'two'), 142),\n",
        " ((u'need', u'5'), 142),\n",
        " ((u'need', u'variety'), 139),\n",
        " ((u'need', u'1'), 136),\n",
        " ((u'class', u'set'), 134),\n",
        " ((u'need', u'class'), 125),\n",
        " ((u'also', u'need'), 114),\n",
        " ((u'need', u'classroom'), 113),\n",
        " ((u'need', u'books'), 110),\n",
        " ((u'need', u'document'), 110),\n",
        " ((u'need', u'10'), 109),\n",
        " ((u'need', u'25'), 109),\n",
        " ((u'need', u'6'), 109),\n",
        " ((u'digital', u'camera'), 109),\n",
        " ((u'school', u'year'), 107),\n",
        " ((u'need', u'20'), 103),\n",
        " ((u'need', u'one'), 98),\n",
        " ((u'art', u'supplies'), 97),\n",
        " ((u'small', u'group'), 91),\n",
        " ((u'need', u'digital'), 91),\n",
        " ((u'reading', u'skills'), 89),\n",
        " ((u'LCD', u'projector'), 89),\n",
        " ((u'need', u'set'), 83),\n",
        " ((u'need', u'math'), 80),\n",
        " ((u'need', u'listening'), 80),\n",
        " ((u'help', u'learn'), 76),\n",
        " ((u'need', u'15'), 75),\n",
        " ((u'30', u'copies'), 74),\n",
        " ((u'need', u'8'), 72),\n",
        " ((u'need', u'laptop'), 71),\n",
        " ((u'pencil', u'sharpener'), 71),\n",
        " ((u'need', u'12'), 71),\n",
        " ((u'They', u'also'), 71),\n",
        " ((u'books', u'including'), 68),\n",
        " ((u'math', u'manipulatives'), 68),\n",
        " ((u'construction', u'paper'), 68),\n",
        " ((u'erase', u'markers'), 68),\n",
        " ((u'need', u'basic'), 65),\n",
        " ((u'laptop', u'computer'), 65),\n",
        " ((u'books', u'read'), 65),\n",
        " ((u'reading', u'comprehension'), 64),\n",
        " ((u'Common', u'Core'), 63),\n",
        " ((u'need', u'LCD'), 63),\n",
        " ((u'math', u'skills'), 62),\n",
        " ((u'enhance', u'learning'), 61),\n",
        " ((u'Time', u'Kids'), 60),\n",
        " ((u'digital', u'cameras'), 59),\n",
        " ((u'high', u'interest'), 57),\n",
        " ((u'bean', u'bag'), 57),\n",
        " ((u'independent', u'reading'), 56),\n",
        " ((u'leveled', u'books'), 56),\n",
        " ((u'copies', u'The'), 55),\n",
        " ((u'2', u'sets'), 55),\n",
        " ((u'colored', u'pencils'), 54),\n",
        " ((u'help', u'us'), 54),\n",
        " ((u'reading', u'writing'), 53),\n",
        " ((u'need', u'hands-on'), 52),\n",
        " ((u'reading', u'levels'), 51),\n",
        " ((u'25', u'copies'), 51),\n",
        " ((u'pocket', u'chart'), 50),\n",
        " ((u'math', u'games'), 49),\n",
        " ((u'book', u'sets'), 49),\n",
        " ((u'improve', u'literacy'), 49),\n",
        " ((u'need', u'three'), 48),\n",
        " ((u'They', u'need'), 48),\n",
        " ((u'need', u'book'), 48),\n",
        " ((u'book', u'titles'), 48)]"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The need statement is structured in such a way that there are a lot of repeated bi-grams, but they are not necessarily carrying a lot of helpful information about the type of need. The data is a bit messy in this format"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Creating smaller sets of first paragraphs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sample_first_paragraph = random.sample(first_paragraph, 1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "big_sample_first_paragraph = random.sample(first_paragraph, 20000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigger_sample_first_paragraph = random.sample(first_paragraph, 100000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Look at the first paragraph most common phrases; based on sample"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##First looked at bi-grams\n",
      "common_words_function(sample_first_paragraph, stop, 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "[((u'Do', u'remember'), 68),\n",
        " ((u'high', u'school'), 57),\n",
        " ((u'would', u'like'), 57),\n",
        " ((u'help', u'students'), 46),\n",
        " ((u'grade', u'students'), 35),\n",
        " ((u'first', u'grade'), 33),\n",
        " ((u'Our', u'school'), 31),\n",
        " ((u'students', u'come'), 30),\n",
        " ((u'special', u'education'), 30),\n",
        " ((u'school', u'students'), 29),\n",
        " ((u'ca', u\"n't\"), 29),\n",
        " ((u'students', u'learn'), 27),\n",
        " ((u'Please', u'help'), 26),\n",
        " ((u'low', u'income'), 25),\n",
        " ((u'students', u'love'), 25),\n",
        " ((u'many', u'students'), 25),\n",
        " ((u'middle', u'school'), 25),\n",
        " ((u'students', u'learning'), 25),\n",
        " ((u'help', u'us'), 24),\n",
        " ((u'every', u'day'), 23),\n",
        " ((u'Many', u'students'), 23),\n",
        " ((u'first', u'time'), 22),\n",
        " ((u'grade', u'teacher'), 22),\n",
        " ((u'public', u'school'), 22),\n",
        " ((u'give', u'students'), 22),\n",
        " ((u'fourth', u'grade'), 21),\n",
        " ((u'students', u'would'), 21),\n",
        " ((u'want', u'students'), 21),\n",
        " ((u'elementary', u'school'), 20),\n",
        " ((u'students', u'school'), 20),\n",
        " ((u'second', u'grade'), 20),\n",
        " ((u'special', u'needs'), 20),\n",
        " ((u'third', u'grade'), 19),\n",
        " ((u'free', u'reduced'), 19),\n",
        " ((u'love', u'reading'), 18),\n",
        " ((u'grade', u'level'), 18),\n",
        " ((u'would', u'love'), 18),\n",
        " ((u'remember', u'first'), 18),\n",
        " ((u'Our', u'students'), 18),\n",
        " ((u'school', u'year'), 17),\n",
        " ((u'grade', u'classroom'), 17),\n",
        " ((u'Have', u'ever'), 17),\n",
        " ((u'nI', u'teach'), 17),\n",
        " ((u'5th', u'grade'), 17),\n",
        " ((u'students', u'get'), 16),\n",
        " ((u'allow', u'students'), 16),\n",
        " ((u'4th', u'grade'), 16),\n",
        " ((u'students', u'able'), 16),\n",
        " ((u'teach', u'students'), 15),\n",
        " ((u'nMy', u'students'), 15),\n",
        " ((u'would', u'allow'), 15),\n",
        " ((u'students', u'reading'), 15),\n",
        " ((u'students', u'use'), 15),\n",
        " ((u'Help', u'us'), 15),\n",
        " ((u'needs', u'students'), 14),\n",
        " ((u'classroom', u'library'), 14),\n",
        " ((u'school', u'district'), 14),\n",
        " ((u'social', u'studies'), 14),\n",
        " ((u'students', u'become'), 14),\n",
        " ((u'students', u'read'), 14),\n",
        " ((u'reading', u'writing'), 14),\n",
        " ((u'students', u'opportunity'), 13),\n",
        " ((u'would', u'help'), 13),\n",
        " ((u'remember', u'like'), 13),\n",
        " ((u'dry', u'erase'), 13),\n",
        " ((u'free', u'lunch'), 13),\n",
        " ((u'read', u'book'), 13),\n",
        " ((u'English', u'Language'), 13),\n",
        " ((u'New', u'York'), 12),\n",
        " ((u'students', u'receive'), 12),\n",
        " ((u'provide', u'students'), 12),\n",
        " ((u'daily', u'basis'), 12),\n",
        " ((u'world', u'around'), 12),\n",
        " ((u'students', u'many'), 12),\n",
        " ((u'many', u'different'), 12),\n",
        " ((u'Our', u'classroom'), 11),\n",
        " ((u'come', u'school'), 11),\n",
        " ((u'eager', u'learn'), 11),\n",
        " ((u'second', u'language'), 11),\n",
        " ((u'Social', u'Studies'), 11),\n",
        " ((u'help', u'make'), 11),\n",
        " ((u'real', u'world'), 11),\n",
        " ((u'today', u\"'s\"), 11),\n",
        " ((u'classroom', u'students'), 11),\n",
        " ((u'Can', u'imagine'), 11),\n",
        " ((u'every', u'student'), 11),\n",
        " ((u'listening', u'center'), 11),\n",
        " ((u'students', u'grades'), 11),\n",
        " ((u'education', u'teacher'), 11),\n",
        " ((u'years', u'old'), 10),\n",
        " ((u'second', u'graders'), 10),\n",
        " ((u'reduced', u'lunch'), 10),\n",
        " ((u'time', u'read'), 10),\n",
        " ((u'These', u'students'), 10),\n",
        " ((u'fifth', u'grade'), 10),\n",
        " ((u'year', u'students'), 10),\n",
        " ((u'students', u'also'), 10),\n",
        " ((u'students', u'feel'), 10),\n",
        " ((u'learning', u'read'), 10),\n",
        " ((u'love', u'read'), 10)]"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given this data, we identified a few key trends - bigrams of need groups such as \"first grade\" or \"special needs\" and then a lot of less helpful bi-grams such as \"Please help\" which don't hold as much information. This data set seemed to be the best option for looking at the text and pulling out actionable information.\n",
      "Next we looked at the most common single words in the dataset to see if there was information there - reading and books were two of the most popular in our sample set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##Then looked at most common individual words\n",
      "common_words_function(sample_first_paragraph, stop, 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "[((u'students',), 1549),\n",
        " ((u'school',), 571),\n",
        " ((u'classroom',), 386),\n",
        " ((u'reading',), 297),\n",
        " ((u'would',), 289),\n",
        " ((u'teach',), 282),\n",
        " ((u'grade',), 275),\n",
        " ((u'books',), 260),\n",
        " ((u'learn',), 242),\n",
        " ((u'learning',), 240),\n",
        " ((u'help',), 224),\n",
        " ((u'read',), 222),\n",
        " ((u'children',), 215),\n",
        " ((u'class',), 212),\n",
        " ((u'year',), 206),\n",
        " ((u'work',), 179),\n",
        " ((u\"'s\",), 154),\n",
        " ((u'like',), 152),\n",
        " ((u'many',), 151),\n",
        " ((u'use',), 151),\n",
        " ((u'teacher',), 144),\n",
        " ((u'time',), 144),\n",
        " ((u'love',), 141),\n",
        " ((u'want',), 137),\n",
        " ((u'first',), 136),\n",
        " ((u'They',), 136),\n",
        " ((u'This',), 134),\n",
        " ((u'able',), 128),\n",
        " ((u'skills',), 125),\n",
        " ((u'one',), 122),\n",
        " ((u'new',), 122),\n",
        " ((u'day',), 121),\n",
        " ((u\"n't\",), 116),\n",
        " ((u'materials',), 114),\n",
        " ((u'get',), 112),\n",
        " ((u'book',), 112),\n",
        " ((u'make',), 112),\n",
        " ((u'technology',), 112),\n",
        " ((u'math',), 110),\n",
        " ((u'also',), 109),\n",
        " ((u'Our',), 109),\n",
        " ((u'come',), 108),\n",
        " ((u'world',), 107),\n",
        " ((u'high',), 105),\n",
        " ((u'It',), 102),\n",
        " ((u'nI',), 100),\n",
        " ((u'English',), 99),\n",
        " ((u'way',), 98),\n",
        " ((u'language',), 95),\n",
        " ((u'us',), 94),\n",
        " ((u'student',), 94),\n",
        " ((u'education',), 92),\n",
        " ((u'writing',), 89),\n",
        " ((u'community',), 85),\n",
        " ((u'program',), 84),\n",
        " ((u'activities',), 83),\n",
        " ((u'graders',), 81),\n",
        " ((u'needs',), 80),\n",
        " ((u'remember',), 79),\n",
        " ((u'readers',), 79),\n",
        " ((u'resources',), 76),\n",
        " ((u'special',), 75),\n",
        " ((u'home',), 75),\n",
        " ((u'level',), 74),\n",
        " ((u'experience',), 73),\n",
        " ((u'teaching',), 72),\n",
        " ((u'see',), 72),\n",
        " ((u'take',), 70),\n",
        " ((u'great',), 69),\n",
        " ((u'These',), 69),\n",
        " ((u'every',), 68),\n",
        " ((u'become',), 68),\n",
        " ((u'years',), 68),\n",
        " ((u'know',), 67),\n",
        " ((u'could',), 67),\n",
        " ((u'Students',), 67),\n",
        " ((u'create',), 67),\n",
        " ((u'important',), 66),\n",
        " ((u'opportunity',), 66),\n",
        " ((u'fun',), 65),\n",
        " ((u'allow',), 65),\n",
        " ((u'science',), 65),\n",
        " ((u'art',), 64),\n",
        " ((u'life',), 64),\n",
        " ((u'kids',), 61),\n",
        " ((u'Do',), 61),\n",
        " ((u'much',), 60),\n",
        " ((u'area',), 60),\n",
        " ((u'library',), 60),\n",
        " ((u'hard',), 60),\n",
        " ((u'well',), 59),\n",
        " ((u'free',), 57),\n",
        " ((u'classes',), 57),\n",
        " ((u'Many',), 57),\n",
        " ((u'kindergarten',), 56),\n",
        " ((u'better',), 56),\n",
        " ((u'without',), 56),\n",
        " ((u'second',), 55),\n",
        " ((u'play',), 54),\n",
        " ((u'small',), 54)]"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Looking at bigrams to identify most common phrases for need and most common audiences"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def most_common_word_phrases(text_corpus, ngram_length=2, extra_stopwords = None,\n",
      "                             nouns_only=False, doc_count=False):\n",
      "\n",
      "    if extra_stopwords is None:\n",
      "        extra_stopwords = []\n",
      "    \n",
      "    stop = stopwords.words(\"english\")\n",
      "    stop += [\".\", \",\", \"(\", \")\", \"'\", '\"', '\\\\n']\n",
      "    stop += extra_stopwords\n",
      "    \n",
      "    counter = Counter()\n",
      "\n",
      "    for i, doc in enumerate(text_corpus):\n",
      "\n",
      "        if i% 25000 == 0:\n",
      "            print >> sys.stderr, \"Processing %i\" % i\n",
      "        \n",
      "        ##Cleaning out extra punctuation formatting from the text\n",
      "        doc = doc.replace(\"\\\\n\", \" \")\n",
      "        \n",
      "        ##If nouns_only is true, than only look at words with the postag \"NN\"\n",
      "        if ngram_length != 'noun_phrases' and nouns_only:\n",
      "            words = [w for w,postag in TextBlob(doc).tags if postag == 'NN']\n",
      "            words = [w for w in words if w not in stop]\n",
      "        elif ngram_length != 'noun_phrases':\n",
      "            words = TextBlob(doc).words\n",
      "            words = [w for w in words if w not in stop]\n",
      "\n",
      "        if ngram_length == 'noun_phrases':\n",
      "            def is_legit(np):\n",
      "                for stopw in stop:\n",
      "                    if stopw in np:\n",
      "                        return False\n",
      "                return True\n",
      "            ##Insures that words are not in the stop words list\n",
      "            N_grams = [np for np in TextBlob(doc).noun_phrases if is_legit(np)]\n",
      "        else:\n",
      "            N_grams = ngrams(words,ngram_length)\n",
      "\n",
      "        if doc_count:\n",
      "            N_grams = list(set(N_grams))\n",
      "        \n",
      "        for gram in N_grams:\n",
      "            counter[gram] += 1\n",
      "\n",
      "    #pprint(counter.most_common(100))\n",
      "    return counter.most_common(250)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Looking at the most common phrases in the sample first paragraph list"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "extra_stop = [\"my\", \"student\", \"students\", 'teach', 'the', 'learn', 'need', 'we',\n",
      "               'us', 'help', 'would', 'I', 'want', 'do', 'remember', 'school', 'our',\n",
      "               'n', 'nI', 'every', 'day', 'have', 'ever', 'love', 'My', 'can', 'imagine',\n",
      "               'We', 'please', 'teacher', 'it', 'It', 'This', 'year', 'many', \"'s\", 'in', 'In',\n",
      "               \"ca\", \"n't\", \"able\", \"Do\", \"like\", \"use\", \"Our\", \"could\", \"A\", \"typical\", \"around\",\n",
      "               \"'ll\", \"The\", \"What\", \"better\", \"make\", 'also', 'get', 'They', 'much','Thank', \n",
      "               'thank', 'These', 'using', 'come', 'ago']\n",
      "               \n",
      "common_phrases = most_common_word_phrases(sample_first_paragraph,\n",
      "                                          ngram_length= 2,\n",
      "                                          extra_stopwords=extra_stop,\n",
      "                                          nouns_only=False,\n",
      "                                          doc_count=True)\n",
      "common_phrases"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Processing 0\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "[((u'first', u'grade'), 35),\n",
        " ((u'low', u'income'), 29),\n",
        " ((u'free', u'reduced'), 26),\n",
        " ((u'reduced', u'lunch'), 21),\n",
        " ((u'first', u'time'), 18),\n",
        " ((u'grade', u'level'), 18),\n",
        " ((u'fifth', u'grade'), 16),\n",
        " ((u'21st', u'century'), 16),\n",
        " ((u'special', u'needs'), 15),\n",
        " ((u'classroom', u'library'), 15),\n",
        " ((u'books', u'read'), 15),\n",
        " ((u'second', u'grade'), 14),\n",
        " ((u'learning', u'environment'), 14),\n",
        " ((u'special', u'education'), 14),\n",
        " ((u'first', u'graders'), 14),\n",
        " ((u'New', u'York'), 13),\n",
        " ((u'high-need', u'community'), 13),\n",
        " ((u'free', u'lunch'), 13),\n",
        " ((u'5th', u'grade'), 13),\n",
        " ((u'English', u'Language'), 13),\n",
        " ((u'social', u'studies'), 13),\n",
        " ((u'reading', u'skills'), 12),\n",
        " ((u'third', u'grade'), 12),\n",
        " ((u'high', u'poverty'), 12),\n",
        " ((u'small', u'group'), 12),\n",
        " ((u'receive', u'free'), 12),\n",
        " ((u'Title', u'1'), 12),\n",
        " ((u'7th', u'grade'), 11),\n",
        " ((u'work', u'hard'), 11),\n",
        " ((u'fourth', u'grade'), 11),\n",
        " ((u'read', u'book'), 11),\n",
        " ((u'inner', u'city'), 11),\n",
        " ((u'math', u'science'), 11),\n",
        " ((u'Social', u'Studies'), 11),\n",
        " ((u'grade', u'class'), 10),\n",
        " ((u'time', u'read'), 10),\n",
        " ((u'Language', u'Arts'), 10),\n",
        " ((u'8th', u'grade'), 10),\n",
        " ((u'excited', u'learning'), 10),\n",
        " ((u'Language', u'Learners'), 9),\n",
        " ((u'small', u'groups'), 9),\n",
        " ((u'grade', u'classroom'), 9),\n",
        " ((u'daily', u'basis'), 9),\n",
        " ((u'6th', u'grade'), 9),\n",
        " ((u'technology', u'classroom'), 9),\n",
        " ((u'sixth', u'grade'), 9),\n",
        " ((u'reading', u'writing'), 9),\n",
        " ((u'hard', u'work'), 9),\n",
        " ((u'learning', u'experience'), 8),\n",
        " ((u'books', u'classroom'), 8),\n",
        " ((u'places', u'go'), 8),\n",
        " ((u'2nd', u'grade'), 8),\n",
        " ((u'learning', u'read'), 8),\n",
        " ((u'learning', u'experiences'), 8),\n",
        " ((u'Dr', u'Seuss'), 8),\n",
        " ((u'read', u'things'), 8),\n",
        " ((u'read', u'write'), 8),\n",
        " ((u'book', u'put'), 8),\n",
        " ((u'know', u'places'), 8),\n",
        " ((u'enough', u'books'), 8),\n",
        " ((u'listening', u'center'), 8),\n",
        " ((u'second', u'language'), 8),\n",
        " ((u'language', u'arts'), 8),\n",
        " ((u'math', u'skills'), 8),\n",
        " ((u'By', u'end'), 8),\n",
        " ((u'grade', u'science'), 8),\n",
        " ((u'read', u'books'), 8),\n",
        " ((u'things', u'know'), 8),\n",
        " ((u'science', u'social'), 8),\n",
        " ((u'group', u'instruction'), 7),\n",
        " ((u'years', u'old'), 7),\n",
        " ((u'second', u'graders'), 7),\n",
        " ((u'materials', u'needed'), 7),\n",
        " ((u'hands-on', u'activities'), 7),\n",
        " ((u'math', u'reading'), 7),\n",
        " ((u'test', u'scores'), 7),\n",
        " ((u'give', u'opportunity'), 7),\n",
        " ((u'North', u'Carolina'), 7),\n",
        " ((u'income', u'families'), 7),\n",
        " ((u'learning', u'styles'), 7),\n",
        " ((u'4th', u'grade'), 7),\n",
        " ((u'enhance', u'learning'), 7),\n",
        " ((u'Common', u'Core'), 7),\n",
        " ((u'Elementary', u'School'), 7),\n",
        " ((u'young', u'people'), 7),\n",
        " ((u'math', u'concepts'), 7),\n",
        " ((u'grade', u'math'), 7),\n",
        " ((u'read', u'aloud'), 7),\n",
        " ((u'yet', u'discovered'), 7),\n",
        " ((u'struggling', u'readers'), 6),\n",
        " ((u'3rd', u'grade'), 6),\n",
        " ((u'general', u'education'), 6),\n",
        " ((u'go', u'Dr'), 6),\n",
        " ((u'develop', u'reading'), 6),\n",
        " ((u'real', u'world'), 6),\n",
        " ((u'language', u'learners'), 6),\n",
        " ((u'limited', u'resources'), 6),\n",
        " ((u'classroom', u'consists'), 6),\n",
        " ((u'South', u'Carolina'), 6),\n",
        " ((u'discovered', u'books'), 6),\n",
        " ((u'grade', u'levels'), 6),\n",
        " ((u'music', u'movement'), 6),\n",
        " ((u'Title', u'One'), 6),\n",
        " ((u'reading', u'books'), 6),\n",
        " ((u'capture', u'imaginations'), 5),\n",
        " ((u'fun', u'exciting'), 5),\n",
        " ((u'small', u'rural'), 5),\n",
        " ((u'one', u'another'), 5),\n",
        " ((u'reading', u'levels'), 5),\n",
        " ((u'reading', u'time'), 5),\n",
        " ((u'Special', u'Education'), 5),\n",
        " ((u'books', u'capture'), 5),\n",
        " ((u'access', u'technology'), 5),\n",
        " ((u'class', u'set'), 5),\n",
        " ((u'real', u'life'), 5),\n",
        " ((u'third', u'graders'), 5),\n",
        " ((u'wide', u'range'), 5),\n",
        " ((u'mathematical', u'concepts'), 5),\n",
        " ((u'overhead', u'projector'), 5),\n",
        " ((u'look', u'forward'), 5),\n",
        " ((u'grade', u'bilingual'), 5),\n",
        " ((u'fifth', u'graders'), 5),\n",
        " ((u'part', u'learning'), 5),\n",
        " ((u'science', u'classroom'), 5),\n",
        " ((u'math', u'problems'), 5),\n",
        " ((u'5th', u'graders'), 5),\n",
        " ((u'grade', u'English'), 5),\n",
        " ((u'learning', u'fun'), 5),\n",
        " ((u'field', u'trip'), 5),\n",
        " ((u'Middle', u'School'), 5),\n",
        " ((u'common', u'core'), 5),\n",
        " ((u'world', u'technology'), 5),\n",
        " ((u'writing', u'center'), 5),\n",
        " ((u'new', u'exciting'), 5),\n",
        " ((u'enjoy', u'reading'), 5),\n",
        " ((u'provide', u'opportunities'), 5),\n",
        " ((u'second', u'teaching'), 4),\n",
        " ((u'San', u'Francisco'), 4),\n",
        " ((u'create', u'learning'), 4),\n",
        " ((u'good', u'books'), 4),\n",
        " ((u'art', u'projects'), 4),\n",
        " ((u'wonderful', u'way'), 4),\n",
        " ((u'learning', u'process'), 4),\n",
        " ((u'York', u'City'), 4),\n",
        " ((u'document', u'camera'), 4),\n",
        " ((u'African', u'American'), 4),\n",
        " ((u'important', u'children'), 4),\n",
        " ((u'educational', u'games'), 4),\n",
        " ((u'fun', u'learning'), 4),\n",
        " ((u'Math', u'Science'), 4),\n",
        " ((u'public', u'elementary'), 4),\n",
        " ((u'literacy', u'skills'), 4),\n",
        " ((u'materials', u'used'), 4),\n",
        " ((u'music', u'room'), 4),\n",
        " ((u'basic', u'math'), 4),\n",
        " ((u'follow', u'along'), 4),\n",
        " ((u'see', u'children'), 4),\n",
        " ((u'meet', u'needs'), 4),\n",
        " ((u'making', u'learning'), 4),\n",
        " ((u'income', u'community'), 4),\n",
        " ((u'brand', u'new'), 4),\n",
        " ((u'ninth', u'grade'), 4),\n",
        " ((u'different', u'ways'), 4),\n",
        " ((u'take', u'pictures'), 4),\n",
        " ((u'Tell', u'forget'), 4),\n",
        " ((u'South', u'Bronx'), 4),\n",
        " ((u'working', u'together'), 4),\n",
        " ((u'basic', u'supplies'), 4),\n",
        " ((u'reading', u'class'), 4),\n",
        " ((u'Have', u'felt'), 4),\n",
        " ((u'full', u'life'), 4),\n",
        " ((u'diverse', u'group'), 4),\n",
        " ((u'fun', u'engaging'), 4),\n",
        " ((u'Second', u'Language'), 4),\n",
        " ((u'problem', u'solving'), 4),\n",
        " ((u'grade', u'Language'), 4),\n",
        " ((u'high', u'needs'), 4),\n",
        " ((u'reading', u'There'), 4),\n",
        " ((u'reading', u'level'), 4),\n",
        " ((u'grade', u'Title'), 4),\n",
        " ((u'hard', u'time'), 4),\n",
        " ((u'English', u'Second'), 4),\n",
        " ((u'guided', u'reading'), 4),\n",
        " ((u'daily', u'lives'), 4),\n",
        " ((u'work', u'together'), 4),\n",
        " ((u'books', u'home'), 4),\n",
        " ((u'learning', u'new'), 4),\n",
        " ((u'learning', u'disabilities'), 4),\n",
        " ((u'dry', u'erase'), 4),\n",
        " ((u'reading', u'comprehension'), 4),\n",
        " ((u'science', u'lab'), 4),\n",
        " ((u'high', u'interest'), 4),\n",
        " ((u'math', u'literacy'), 4),\n",
        " ((u'classroom', u'read'), 4),\n",
        " ((u'reading', u'instruction'), 4),\n",
        " ((u'children', u'learning'), 4),\n",
        " ((u'opportunity', u'explore'), 4),\n",
        " ((u'sight', u'words'), 4),\n",
        " ((u'learning', u'community'), 4),\n",
        " ((u'computer', u'lab'), 4),\n",
        " ((u'literacy', u'centers'), 4),\n",
        " ((u'learning', u'centers'), 4),\n",
        " ((u'find', u'books'), 4),\n",
        " ((u'title', u'one'), 4),\n",
        " ((u'life', u'long'), 4),\n",
        " ((u'English', u'second'), 4),\n",
        " ((u'digital', u'camera'), 4),\n",
        " ((u'feel', u'way'), 4),\n",
        " ((u'thinking', u'skills'), 4),\n",
        " ((u'For', u'example'), 4),\n",
        " ((u'classroom', u'children'), 4),\n",
        " ((u'qualifying', u'free'), 4),\n",
        " ((u'learning', u'time'), 4),\n",
        " ((u'LCD', u'projector'), 4),\n",
        " ((u'provide', u'wonderful'), 4),\n",
        " ((u'incorporate', u'technology'), 4),\n",
        " ((u'qualify', u'free'), 4),\n",
        " ((u'books', u'books'), 4),\n",
        " ((u'favorite', u'books'), 4),\n",
        " ((u'biggest', u'challenge'), 4),\n",
        " ((u'wide', u'variety'), 4),\n",
        " ((u'writing', u'skills'), 4),\n",
        " ((u'Many', u'yet'), 3),\n",
        " ((u'rest', u'class'), 3),\n",
        " ((u'new', u'things'), 3),\n",
        " ((u'background', u'knowledge'), 3),\n",
        " ((u'wonderful', u'learning'), 3),\n",
        " ((u'books', u'meet'), 3),\n",
        " ((u'That', u'exactly'), 3),\n",
        " ((u'learning', u'activities'), 3),\n",
        " ((u'third', u'fourth'), 3),\n",
        " ((u'School', u'low'), 3),\n",
        " ((u'books', u'CD'), 3),\n",
        " ((u'learning', u'English'), 3),\n",
        " ((u'camera', u'allow'), 3),\n",
        " ((u'drives', u'save'), 3),\n",
        " ((u'classroom', u'benefit'), 3),\n",
        " ((u'one', u'way'), 3),\n",
        " ((u'reading', u'book'), 3),\n",
        " ((u'great', u'time'), 3),\n",
        " ((u'rest', u'lives'), 3),\n",
        " ((u'technology', u'order'), 3),\n",
        " ((u'grade', u'read'), 3),\n",
        " ((u'receiving', u'free'), 3),\n",
        " ((u'across', u'country'), 3),\n",
        " ((u'grade', u'Science'), 3),\n",
        " ((u'classroom', u'reading'), 3),\n",
        " ((u'key', u'learning'), 3),\n",
        " ((u'critical', u'thinking'), 3),\n",
        " ((u'11', u'years'), 3)]"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Looking at most common phrases across all first paragraphs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "common_phrases = most_common_word_phrases(first_paragraph,\n",
      "                                          ngram_length= 2,\n",
      "                                          extra_stopwords=extra_stop,\n",
      "                                          nouns_only=False,\n",
      "                                          doc_count=True)\n",
      "common_phrases"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Processing 0\n",
        "Processing 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 50000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 75000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 100000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 125000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 150000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 175000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 200000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 225000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 250000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 275000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 300000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 325000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 350000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 375000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 400000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 425000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 450000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 475000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 500000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 525000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 550000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 575000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 600000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 625000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 650000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 675000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 700000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 725000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Processing 750000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "[((u'first', u'grade'), 20473),\n",
        " ((u'low', u'income'), 15927),\n",
        " ((u'special', u'education'), 15154),\n",
        " ((u'free', u'reduced'), 15081),\n",
        " ((u'first', u'time'), 14987),\n",
        " ((u'grade', u'level'), 14233),\n",
        " ((u'second', u'grade'), 13184),\n",
        " ((u'third', u'grade'), 12919),\n",
        " ((u'reduced', u'lunch'), 11993),\n",
        " ((u'special', u'needs'), 11571),\n",
        " ((u'classroom', u'library'), 10947),\n",
        " ((u'English', u'Language'), 10177),\n",
        " ((u'reading', u'writing'), 9727),\n",
        " ((u'New', u'York'), 9557),\n",
        " ((u'5th', u'grade'), 9435),\n",
        " ((u'free', u'lunch'), 9385),\n",
        " ((u'receive', u'free'), 9110),\n",
        " ((u'high', u'poverty'), 8993),\n",
        " ((u'social', u'studies'), 8859),\n",
        " ((u'fifth', u'grade'), 8687),\n",
        " ((u'inner', u'city'), 8609),\n",
        " ((u'first', u'graders'), 8592),\n",
        " ((u'fourth', u'grade'), 8049),\n",
        " ((u'8th', u'grade'), 7853),\n",
        " ((u'reading', u'skills'), 7768),\n",
        " ((u'read', u'book'), 7718),\n",
        " ((u'books', u'read'), 7604),\n",
        " ((u'Title', u'1'), 7358),\n",
        " ((u'Language', u'Learners'), 7344),\n",
        " ((u'21st', u'century'), 7148),\n",
        " ((u'technology', u'classroom'), 7024),\n",
        " ((u'grade', u'classroom'), 7008),\n",
        " ((u'grade', u'class'), 6998),\n",
        " ((u'listening', u'center'), 6746),\n",
        " ((u'daily', u'basis'), 6671),\n",
        " ((u'4th', u'grade'), 6615),\n",
        " ((u'high-need', u'community'), 6467),\n",
        " ((u'learning', u'read'), 6327),\n",
        " ((u'6th', u'grade'), 6099),\n",
        " ((u'English', u'language'), 6025),\n",
        " ((u'second', u'language'), 5960),\n",
        " ((u'small', u'groups'), 5828),\n",
        " ((u'read', u'books'), 5807),\n",
        " ((u'small', u'group'), 5619),\n",
        " ((u'work', u'hard'), 5597),\n",
        " ((u'time', u'read'), 5580),\n",
        " ((u'Language', u'Arts'), 5512),\n",
        " ((u'3rd', u'grade'), 5442),\n",
        " ((u'budget', u'cuts'), 5427),\n",
        " ((u'language', u'learners'), 5397),\n",
        " ((u'Social', u'Studies'), 5341),\n",
        " ((u'reading', u'level'), 5311),\n",
        " ((u'2nd', u'grade'), 5308),\n",
        " ((u'Dr', u'Seuss'), 5266),\n",
        " ((u'math', u'science'), 5213),\n",
        " ((u'language', u'arts'), 5133),\n",
        " ((u'learning', u'environment'), 4958),\n",
        " ((u'real', u'world'), 4885),\n",
        " ((u'math', u'skills'), 4865),\n",
        " ((u'North', u'Carolina'), 4798),\n",
        " ((u'second', u'graders'), 4795),\n",
        " ((u'years', u'old'), 4763),\n",
        " ((u'give', u'opportunity'), 4734),\n",
        " ((u'reading', u'books'), 4642),\n",
        " ((u'7th', u'grade'), 4488),\n",
        " ((u'high', u'needs'), 4427),\n",
        " ((u'learning', u'fun'), 4413),\n",
        " ((u'reading', u'levels'), 4320),\n",
        " ((u'read', u'write'), 4207),\n",
        " ((u'High', u'School'), 4195),\n",
        " ((u'books', u'classroom'), 4190),\n",
        " ((u'Special', u'Education'), 4188),\n",
        " ((u'learning', u'experience'), 4181),\n",
        " ((u'third', u'graders'), 4177),\n",
        " ((u'access', u'technology'), 4172),\n",
        " ((u'hands-on', u'activities'), 4138),\n",
        " ((u'sixth', u'grade'), 4049),\n",
        " ((u'document', u'camera'), 3984),\n",
        " ((u'reading', u'math'), 3950),\n",
        " ((u'book', u'read'), 3864),\n",
        " ((u'Common', u'Core'), 3807),\n",
        " ((u'learning', u'disabilities'), 3748),\n",
        " ((u'class', u'set'), 3747),\n",
        " ((u'enhance', u'learning'), 3741),\n",
        " ((u'qualify', u'free'), 3719),\n",
        " ((u'struggling', u'readers'), 3665),\n",
        " ((u'grade', u'English'), 3648),\n",
        " ((u'basic', u'supplies'), 3621),\n",
        " ((u'reading', u'comprehension'), 3614),\n",
        " ((u'enjoy', u'reading'), 3609),\n",
        " ((u'things', u'know'), 3573),\n",
        " ((u'good', u'book'), 3545),\n",
        " ((u'learning', u'experiences'), 3531),\n",
        " ((u'places', u'go'), 3499),\n",
        " ((u'meet', u'needs'), 3440),\n",
        " ((u'great', u'way'), 3427),\n",
        " ((u'know', u'places'), 3398),\n",
        " ((u'read', u'things'), 3394),\n",
        " ((u'computer', u'lab'), 3336),\n",
        " ((u'1st', u'grade'), 3331),\n",
        " ((u'excited', u'learning'), 3323),\n",
        " ((u'brand', u'new'), 3308),\n",
        " ((u'York', u'City'), 3305),\n",
        " ((u'biggest', u'challenge'), 3302),\n",
        " ((u'independent', u'reading'), 3283),\n",
        " ((u'take', u'home'), 3274),\n",
        " ((u'math', u'concepts'), 3267),\n",
        " ((u'literacy', u'skills'), 3267),\n",
        " ((u'7th', u'8th'), 3255),\n",
        " ((u'read', u'aloud'), 3224),\n",
        " ((u'writing', u'skills'), 3204),\n",
        " ((u'United', u'States'), 3202),\n",
        " ((u'classroom', u'full'), 3178),\n",
        " ((u'income', u'families'), 3177),\n",
        " ((u'limited', u'resources'), 3168),\n",
        " ((u'working', u'hard'), 3150),\n",
        " ((u'English', u'second'), 3141),\n",
        " ((u'hard', u'work'), 3122),\n",
        " ((u'grade', u'science'), 3113),\n",
        " ((u'work', u'together'), 3105),\n",
        " ((u'general', u'education'), 3098),\n",
        " ((u'African', u'American'), 3095),\n",
        " ((u'grade', u'math'), 3090),\n",
        " ((u'digital', u'camera'), 3076),\n",
        " ((u'By', u'end'), 3059),\n",
        " ((u'hands-on', u'learning'), 3052),\n",
        " ((u'science', u'social'), 3039),\n",
        " ((u'Elementary', u'School'), 3039),\n",
        " ((u'real', u'life'), 3034),\n",
        " ((u'grade', u'levels'), 3029),\n",
        " ((u'improve', u'reading'), 3011),\n",
        " ((u'problem', u'solving'), 3009),\n",
        " ((u'current', u'events'), 2981),\n",
        " ((u'Title', u'One'), 2929),\n",
        " ((u'learning', u'styles'), 2923),\n",
        " ((u'look', u'forward'), 2909),\n",
        " ((u'reading', u'materials'), 2901),\n",
        " ((u'new', u'books'), 2877),\n",
        " ((u'new', u'things'), 2862),\n",
        " ((u'social', u'skills'), 2857),\n",
        " ((u'reading', u'time'), 2838),\n",
        " ((u'books', u'home'), 2824),\n",
        " ((u'learning', u'new'), 2807),\n",
        " ((u'books', u'reading'), 2779),\n",
        " ((u'fifth', u'graders'), 2777),\n",
        " ((u'reading', u'book'), 2776),\n",
        " ((u'book', u'put'), 2764),\n",
        " ((u'grade', u'Title'), 2735),\n",
        " ((u'grade', u'reading'), 2732),\n",
        " ((u'learning', u'English'), 2724),\n",
        " ((u'become', u'readers'), 2720),\n",
        " ((u'best', u'way'), 2705),\n",
        " ((u'high', u'interest'), 2666),\n",
        " ((u'critical', u'thinking'), 2651),\n",
        " ((u'young', u'children'), 2646),\n",
        " ((u'art', u'supplies'), 2644),\n",
        " ((u'subject', u'areas'), 2624),\n",
        " ((u'dry', u'erase'), 2605),\n",
        " ((u'LCD', u'projector'), 2603),\n",
        " ((u'fun', u'learning'), 2562),\n",
        " ((u'two', u'years'), 2556),\n",
        " ((u'wide', u'range'), 2542),\n",
        " ((u'Have', u'tried'), 2540),\n",
        " ((u'small', u'rural'), 2531),\n",
        " ((u'eighth', u'grade'), 2500),\n",
        " ((u'math', u'class'), 2486),\n",
        " ((u'Middle', u'School'), 2481),\n",
        " ((u'physical', u'education'), 2472),\n",
        " ((u'children', u'read'), 2465),\n",
        " ((u'fourth', u'graders'), 2421),\n",
        " ((u'engaged', u'learning'), 2409),\n",
        " ((u'learning', u'process'), 2394),\n",
        " ((u'low', u'socioeconomic'), 2387),\n",
        " ((u'poverty', u'level'), 2361),\n",
        " ((u'South', u'Bronx'), 2353),\n",
        " ((u'speak', u'English'), 2352),\n",
        " ((u'poverty', u'area'), 2352),\n",
        " ((u'provide', u'opportunity'), 2346),\n",
        " ((u'hard', u'time'), 2334),\n",
        " ((u'5th', u'graders'), 2334),\n",
        " ((u'fun', u'exciting'), 2331),\n",
        " ((u'whole', u'class'), 2320),\n",
        " ((u'new', u'technology'), 2316),\n",
        " ((u'low', u'socio-economic'), 2311),\n",
        " ((u'motor', u'skills'), 2295),\n",
        " ((u'see', u'world'), 2289),\n",
        " ((u'At', u'end'), 2264),\n",
        " ((u'high', u'community'), 2243),\n",
        " ((u'science', u'class'), 2229),\n",
        " ((u'wide', u'variety'), 2224),\n",
        " ((u'9th', u'grade'), 2221),\n",
        " ((u'life', u'long'), 2216),\n",
        " ((u'grade', u'high'), 2204),\n",
        " ((u'kindergarten', u'classroom'), 2182),\n",
        " ((u'go', u'Dr'), 2176),\n",
        " ((u'different', u'ways'), 2161),\n",
        " ((u'spend', u'time'), 2149),\n",
        " ((u'practice', u'reading'), 2147),\n",
        " ((u'lack', u'resources'), 2141),\n",
        " ((u'one', u'another'), 2139),\n",
        " ((u'new', u'exciting'), 2135),\n",
        " ((u'math', u'reading'), 2121),\n",
        " ((u'test', u'scores'), 2121),\n",
        " ((u'important', u'part'), 2114),\n",
        " ((u'video', u'games'), 2099),\n",
        " ((u'learning', u'opportunities'), 2093),\n",
        " ((u'access', u'books'), 2086),\n",
        " ((u'guided', u'reading'), 2085),\n",
        " ((u'education', u'classroom'), 2082),\n",
        " ((u'whole', u'group'), 2071),\n",
        " ((u'diverse', u'population'), 2061),\n",
        " ((u'Every', u'morning'), 2042),\n",
        " ((u'language', u'skills'), 2041),\n",
        " ((u'seventh', u'grade'), 2039),\n",
        " ((u'readers', u'writers'), 2035),\n",
        " ((u'urban', u'district'), 2029),\n",
        " ((u'Second', u'Language'), 2024),\n",
        " ((u'South', u'Carolina'), 2014),\n",
        " ((u'thinking', u'skills'), 2013),\n",
        " ((u'library', u'books'), 2009),\n",
        " ((u'Your', u'donation'), 2005),\n",
        " ((u'excited', u'reading'), 2001),\n",
        " ((u'hands', u'activities'), 1999),\n",
        " ((u'explore', u'world'), 1992),\n",
        " ((u'due', u'lack'), 1989),\n",
        " ((u'considering', u'proposal'), 1986),\n",
        " ((u'economically', u'disadvantaged'), 1986),\n",
        " ((u'something', u'new'), 1980),\n",
        " ((u'first', u'teaching'), 1978),\n",
        " ((u'diverse', u'group'), 1977),\n",
        " ((u'grade', u'low'), 1971),\n",
        " ((u'Tell', u'forget'), 1967),\n",
        " ((u'rural', u'community'), 1964),\n",
        " ((u'read', u'read'), 1959),\n",
        " ((u'reading', u'grade'), 1958),\n",
        " ((u'21st', u'Century'), 1957),\n",
        " ((u'8th', u'graders'), 1950),\n",
        " ((u'English', u'Second'), 1947),\n",
        " ((u'income', u'area'), 1944),\n",
        " ((u'fine', u'motor'), 1941),\n",
        " ((u'become', u'successful'), 1941),\n",
        " ((u'For', u'example'), 1938),\n",
        " ((u'comprehension', u'skills'), 1933),\n",
        " ((u'4th', u'5th'), 1915),\n",
        " ((u'face', u'challenges'), 1913),\n",
        " ((u'low-income', u'families'), 1913),\n",
        " ((u'overhead', u'projector'), 1908),\n",
        " ((u'work', u'small'), 1896),\n",
        " ((u'world', u'technology'), 1893),\n",
        " ((u'outside', u'classroom'), 1889)]"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have a better understanding of the bigrams within our dataset, we can pull out patterns and identify areas of need within the corpus."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"common_phrases.pkl\", \"w\") as picklefile:\n",
      "    pickle.dump(common_phrases, picklefile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"common_phrases.pkl\", \"r\") as picklefile:\n",
      "    common_phrases = pickle.load(picklefile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Group by types students targeted:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##Function to look at commonalities in common_phrases\n",
      "##Did not use this script as we instead created a function that leveraged regular expressions.  \n",
      "def commonality_function(commonlist, keywords, notwords):\n",
      "    \n",
      "    keywords = set(keywords)\n",
      "    notwords = set(notwords)\n",
      "    keyword_list = []\n",
      "    doc_count = 0\n",
      "    \n",
      "    for words, count in commonlist:\n",
      "        word1, word2 = words\n",
      "        if (word1 not in notwords) \\\n",
      "            and (word2 not in notwords) \\\n",
      "            and ((word1 in keywords) or (word2 in keywords)):\n",
      "            keyword_list.append(words)\n",
      "            doc_count += count    \n",
      "    print keyword_list\n",
      "    print doc_count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "def regex_find_graders(commonlist, re_pattern):\n",
      "    matches = []\n",
      "    tot_count = 0\n",
      "    \n",
      "    pattern = re.compile(re_pattern)\n",
      "    ##compiles a regular expression object so it can be used for matching\n",
      "    for words, count in commonlist:\n",
      "        phrase = \" \".join(words)\n",
      "        if re.match(pattern, phrase):\n",
      "            ##compares the regular expression pattern to the phrase in the text\n",
      "            matches.append(phrase)\n",
      "            tot_count += count\n",
      "            ##counts the similar phrases\n",
      "            \n",
      "    return matches, tot_count\n",
      "\n",
      "\n",
      "\n",
      "patterns_all = [\"[Pp]re-[Kk]\",\n",
      "            \"[Pp]reschool\",\n",
      "            \"[Kk]indergarten\",\n",
      "            \"[Ss]pecial\",\n",
      "            \"[Aa]utism\",\n",
      "            \"^(1st|[Ff]irst) [Gg]rade.*\",\n",
      "            \"^(2nd|[Ss]econd) [Gg]rade.*\",\n",
      "            \"^(3rd|[Tt]hird) [Gg]rade.*\",\n",
      "            \"^(4th|[Ff]ourth) [Gg]rade.*\",\n",
      "            \"^(5th|[Ff]ifth) [Gg]rade.*\",\n",
      "            \"^(6th|[Ss]ixth) [Gg]rade.*\",\n",
      "            \"^(7th|[Ss]eventh) [Gg]rade.*\",\n",
      "            \"^(8th|[Ee]ighth) [Gg]rade.*\",\n",
      "            \"^(9th|[Nn]inth|[Ff]reshman) [Gg]rade.*\",\n",
      "            \"^(10th|[Tt]enth|[Ss]ophmore) [Gg]rade.*\",\n",
      "            \"^(11th|[Ee]leventh|[Jj]unior) [Gg]rade.*\",\n",
      "            \"^(12th|[Tt]welfth|[Ss]enior) [Gg]rade.*\",\n",
      "            \"[Ee]lementary\",\n",
      "            \"[Mm]iddle [Ss]chool.*\",\n",
      "            \"[Hh]igh [Ss]chool.*\",\n",
      "            \"[Hh]igh [Pp]overty\",\n",
      "            \"[Ll]ow\",\n",
      "            \"[Rr]ural\",\n",
      "            \"[Uu]rban\",\n",
      "            \"[Ii]nner [Cc]ity\"\n",
      "            ]\n",
      "\n",
      "##created a list that took out any patterns that weren't found in the corpus\n",
      "patterns_clean = [\"[Kk]indergarten\",\n",
      "            \"[Ss]pecial\",\n",
      "            \"^(1st|[Ff]irst) [Gg]rade.*\",\n",
      "            \"^(2nd|[Ss]econd) [Gg]rade.*\",\n",
      "            \"^(3rd|[Tt]hird) [Gg]rade.*\",\n",
      "            \"^(4th|[Ff]ourth) [Gg]rade.*\",\n",
      "            \"^(5th|[Ff]ifth) [Gg]rade.*\",\n",
      "            \"^(6th|[Ss]ixth) [Gg]rade.*\",\n",
      "            \"^(7th|[Ss]eventh) [Gg]rade.*\",\n",
      "            \"^(8th|[Ee]ighth) [Gg]rade.*\",\n",
      "            \"^(9th|[Nn]inth|[Ff]reshman) [Gg]rade.*\",\n",
      "            \"[Ee]lementary\",\n",
      "            \"[Mm]iddle [Ss]chool.*\",\n",
      "            \"[Hh]igh [Ss]chool.*\",\n",
      "            \"[Hh]igh [Pp]overty| [Ll]ow\",\n",
      "            \"[Rr]ural\",\n",
      "            \"^[Uu]rban|[Ii]nner [Cc]it.*\",\n",
      "            \"^[Ee]nglish ([Ll]anguage|[Ss]econd)\"\n",
      "            ]\n",
      "\n",
      "keyword_list = []\n",
      "##creating a list of the keywords in addition to printing them\n",
      "for pattern in patterns_clean:\n",
      "    print regex_find_graders(common_phrases, pattern)\n",
      "    keyword_list.append(regex_find_graders(common_phrases, pattern))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "([u'kindergarten classroom'], 2182)\n",
        "([u'special education', u'special needs', u'Special Education'], 30913)\n",
        "([u'first grade', u'first graders', u'1st grade'], 32396)\n",
        "([u'second grade', u'2nd grade', u'second graders'], 23287)\n",
        "([u'third grade', u'3rd grade', u'third graders'], 22538)\n",
        "([u'fourth grade', u'4th grade', u'fourth graders'], 17085)\n",
        "([u'5th grade', u'fifth grade', u'fifth graders', u'5th graders'], 23233)\n",
        "([u'6th grade', u'sixth grade'], 10148)\n",
        "([u'7th grade', u'seventh grade'], 6527)\n",
        "([u'8th grade', u'eighth grade', u'8th graders'], 12303)\n",
        "([u'9th grade'], 2221)\n",
        "([u'Elementary School'], 3039)\n",
        "([u'Middle School'], 2481)\n",
        "([u'High School'], 4195)\n",
        "([u'high poverty'], 8993)\n",
        "([u'rural community'], 1964)\n",
        "([u'inner city', u'urban district'], 10638)\n",
        "([u'English Language', u'English language', u'English second', u'English Second'], 21290)\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##Sorted list of bi-grams in descending order to be able to create sorted bar chart\n",
      "sorted_keyword_list = sorted(keyword_list, key=lambda x: x[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Using Plotly to graph the common phrases"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import plotly\n",
      "import plotly.plotly as py\n",
      "import plotly.tools as tls\n",
      "py.sign_in(\"LylePayne\", \"swrmhfhj83\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from plotly.graph_objs import Histogram, Bar"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Creating a sorted bar chart"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "py.plot([Bar(x = [x[1] for x in sorted_keyword_list]), ])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "u'https://plot.ly/~LylePayne/56'"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Creating an unsorted bar chart to make it easier to see grade levels in order"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "py.plot([Bar(x = [x[1] for x in keyword_list]), ])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "u'https://plot.ly/~LylePayne/77'"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Creating function to create new subset corpus only focused on a single need group so as to be able to identify clusters of need within that target group"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def new_corpus(paragraphlist, subsetkeywords):\n",
      "\n",
      "    doc_count = 0\n",
      "    doc_list = []\n",
      "\n",
      "    for document in paragraphlist:\n",
      "        for keyword in subsetkeywords:\n",
      "            if keyword in document:\n",
      "                doc_count += 1\n",
      "                doc_list.append(document)\n",
      "        \n",
      "    print \"Doc_Count:\", doc_count\n",
      "    return doc_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "special_need_list = [\"special needs\", \"Special Needs\", \n",
      "                     \"SPECIAL NEEDS\", \"special education\", \n",
      "                     \"Special Education\", \n",
      "                     \"special ed\", \"Special Ed\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Create sample sets of first paragraphs with a focus on special needs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "specialneeds_corpus = new_corpus(big_sample_first_paragraph, special_need_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Doc_Count: 1310\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "specialneeds_corpus2 = new_corpus(bigger_sample_first_paragraph, special_need_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Doc_Count: 6919\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "specialneeds_corpus = new_corpus(first_paragraph, special_need_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Doc_Count: 52357\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Adapted function to return a dictionary which contains the cluster length, the centroid paragraph and the top TFIDF words and their scores; allows us to experiment with different models and parameters and evaluate the clusters which are returned"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clustering_function(subsetcorpus, clustermodel, n_clusters, n_init, special_stopwords=[]):\n",
      "    \n",
      "    clean_corpus =[]\n",
      "    #Remove additional punctuation from essays\n",
      "    for doc in subsetcorpus:\n",
      "        clean_corpus.append(doc.replace(\"\\r\\\\n\\r\\\\n\", \" \").replace('\\u201c','').replace('\\u201d',''))\n",
      "    \n",
      "    #Ensure there are no duplicate entries in the list of first paragraphs\n",
      "    clean_set = list(set(clean_corpus))\n",
      "    \n",
      "    stop = stopwords.words(\"english\") \n",
      "    stop += [\".\", \",\", \"(\", \")\", \"'\", '\"', '\\\\n', \"\\n\", '\\n\"']\n",
      "    stop += special_stopwords\n",
      "    \n",
      "    #Use TF-IDF Vectorizer to find word scores\n",
      "    vectorizer = TfidfVectorizer(stop_words=stop)\n",
      "    doc_vectors = vectorizer.fit_transform(clean_set)\n",
      "    \n",
      "    est = clustermodel(n_clusters = n_clusters, n_init=n_init, )\n",
      "    \n",
      "    model = est.fit(doc_vectors)\n",
      "    labels = model.labels_\n",
      "    \n",
      "    sorted_labels = sorted(labels)\n",
      "    \n",
      "    cluster_centers = model.cluster_centers_\n",
      "    cluster_lengths = Counter(labels)\n",
      "    \n",
      "    clustered_text = [(y,x) for (y,x) in sorted(zip(labels, clean_set))]\n",
      "    \n",
      "    list_of_ask_clusters = []\n",
      "    \n",
      "    TFIDFlist= cluster_top_TFIDF(clustered_text)\n",
      "    \n",
      "    for i, cluster_center in enumerate(cluster_centers):\n",
      "                \n",
      "        values = doc_vectors[i].data\n",
      "        column = doc_vectors[i].indices\n",
      "        \n",
      "        distances = pairwise_distances(doc_vectors, cluster_center, metric=\"cosine\")\n",
      "             \n",
      "        index = min(enumerate(distances), key=itemgetter(1))[0]\n",
      "        \n",
      "        #created a dictionary for each cluster\n",
      "        cluster_details = {'cluster_length': cluster_lengths[i], \n",
      "                           'centroid_paragraph' : clean_set[index],\n",
      "                           'TFIDFlist': TFIDFlist[i]}\n",
      "        list_of_ask_clusters.append(cluster_details)\n",
      "        \n",
      "    return list_of_ask_clusters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Function called within the function above to re-vectorize the clustered text and return the top TFIDF words and scores"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cluster_top_TFIDF(clusteredtext):\n",
      "    #Creates a unduplicated list of cluster labels\n",
      "    label_set = set([i for i,text in clusteredtext])\n",
      "    \n",
      "    dict_of_clustered_texts = defaultdict(str)\n",
      "    \n",
      "    #Creates a dictionary where each key is a cluster label and the paragraphs in that cluster are all combined into the value\n",
      "    for cluster_index, text in clusteredtext:\n",
      "        dict_of_clustered_texts[cluster_index] += text\n",
      "\n",
      "    list_of_clustered_text = dict_of_clustered_texts.values()\n",
      "    \n",
      "    stop = stopwords.words(\"english\")\n",
      "    stop += [\"special\", \"grade\", \"school\", \"students\", \"education\", \n",
      "             \"need\", \"would\", \"teach\", \"needs\", \"class\", \"classroom\", \"teacher\", \n",
      "             \"help\", \"many\", \"come\", \"children\", \"one\", \"skills\", \"work\", \"ni\", \"like\", \"learning\"]\n",
      "    stop += extra_stop\n",
      "    second_vectorizer = TfidfVectorizer(stop_words=stop)\n",
      "    #Use TF-IDF Vectorizer a second time within the clustered documents to find the most important words in each cluster\n",
      "    vector_cluster = second_vectorizer.fit_transform(list_of_clustered_text)\n",
      "    \n",
      "    TFIDF_list = []\n",
      "    \n",
      "    for i in label_set:\n",
      "        \n",
      "        value = vector_cluster[i].data\n",
      "        column = vector_cluster[i].indices\n",
      "    \n",
      "        sorted_column_index = [x for (y,x) in sorted(zip(value, column), reverse=True)]\n",
      "        \n",
      "        TFIDF_cluster_list = []\n",
      "    \n",
      "        for x in sorted_column_index[:5]:\n",
      "            TFIDF_name = second_vectorizer.get_feature_names()[x]\n",
      "            TFIDF_score = vector_cluster[i,x]\n",
      "            TFIDF_cluster_list.append((TFIDF_name, TFIDF_score))\n",
      "        \n",
      "        TFIDF_list.append(TFIDF_cluster_list)\n",
      "    \n",
      "    return TFIDF_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kmeans_cluster_details = clustering_function(specialneeds_corpus, KMeans, 30, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 440
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in enumerate(kmeans_cluster_details):\n",
      "    pprint(i)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(0,\n",
        " {'TFIDFlist': [(u'writing', 0.18882950848025706),\n",
        "                (u'time', 0.17968976187060143),\n",
        "                (u'reading', 0.17898670443908946),\n",
        "                (u'materials', 0.16510132016672802),\n",
        "                (u'new', 0.12604533538402371)],\n",
        "  'centroid_paragraph': u'\"I am a special education teacher at CICS Wrightwood campus.  I have a new self-contained classroom. I am asking for resources for my 8 students.  At the moment, I am making due with what I have.  However, I feel my students are missing out.  \\r\\\\n\\r\\\\nThe school I work for is a college preparatory school.  Many resources are offered to the students.  I would like to give my students the same opportunity.  I realize with a new room I will start from scratch.  I did get the necessary pens, pencils, etc.  However, I am looking to get math manipulatives.  \\r\\\\n\\r\\\\nI look forward to the day my students can use a Hands-on Math Supply Center.  It is one thing to teach math from the chalkboard.  When my students can use hands-on manipulatives my lessons will become easier to grasp.  I can imagine my students using Place Value Training Kits.  I expect that their experience with math will be more vibrant and interesting.  I feel I can plan more creative lessons for them.\\r\\\\n \\r\\\\nI also need some writing supplies.  Right now I give my students a writing prompt every day.  One that I make up.  I would like them to have more prompts and ones that challenge their minds. The students\\' imagination must be cultivated.  I want to encourage them to think.  When I show them descriptive writing materials I know their imaginations can soar.\\r\\\\n\\r\\\\nMy students have different needs and I try to provide for their individual needs.  Write and Wipe Number Lines will help some of my students tremendously.  I can use manipulatives to teach my students the concept of money, also.  \\r\\\\n\\r\\\\nMy students are bright and full of promise.  I want to be able to be a part of their growth.  All of the manipulatives and materials I am asking for will benefit them greatly.\\r\\\\n\\r\\\\nI do not want my students to be forgotten or left out.  I want to work for them to receive the materials they need.  I can see their minds working on the Problem of the Week.  As they use their minds to correctly answer the question it will give me great pride to know I was helpful in some way.\\r\\\\n\\r\\\\nThere is something I would like to try in order to help with organization.  I would like to color code my students\\' folders and notebooks with each subject I teach.  Math, language arts, social science and science are the subjects I teach. \\r\\\\n\\r\\\\nThere are many more ideas I would like to use with my students.  All of the materials I  have asked for will help to get my ideas across.\\r\\\\n\\r\\\\nIn conclusion, I would like to thank, in advance, any and all individuals who find this a worthwhile proposal.  Thank you, from myself and my students.   \\r\\\\n\\r\\\\n \\r\\\\n\\r\\\\n  \"',\n",
        "  'cluster_length': 4777})\n",
        "(1,\n",
        " {'TFIDFlist': [(u'math', 0.84626706121254747),\n",
        "                (u'concepts', 0.11325155711937354),\n",
        "                (u'manipulatives', 0.1012261040603485),\n",
        "                (u'materials', 0.10001681755617464),\n",
        "                (u'hands', 0.095857327979169266)],\n",
        "  'centroid_paragraph': u'\"Math, math, we love math! My kindergarten students love to use the math manipulatives that we have in the classroom, but they want more! The students\\' resources at home are sometimes limited, so providing opportunities for them in the classroom is important. My classroom, which is composed of regular education students and special education students will benefit from new hands on materials to learn new math concepts. \"',\n",
        "  'cluster_length': 1499})\n",
        "(2,\n",
        " {'TFIDFlist': [(u'income', 0.63175231144754029),\n",
        "                (u'low', 0.6276337487391731),\n",
        "                (u'high', 0.14982017102860137),\n",
        "                (u'families', 0.126944402979073),\n",
        "                (u'area', 0.11646950066950393)],\n",
        "  'centroid_paragraph': u'\"I teach Special Education to 7th grade students.  My school is low income.\"',\n",
        "  'cluster_length': 1793})\n",
        "(3,\n",
        " {'TFIDFlist': [(u'program', 0.34399030238344996),\n",
        "                (u'ethnicity', 0.18682360896706221),\n",
        "                (u'follows', 0.16661506052828262),\n",
        "                (u'therapeutic', 0.16066578011019045),\n",
        "                (u'asian', 0.16066578011019045)],\n",
        "  'centroid_paragraph': u'\"I work at a special education school, servicing emotionally disturbed, learning disabled, and other special education classified students. Our students come from low socioeconomic backgrounds. Over 95% of the student body is eligible for free lunch and over 90% receive public assistance. These factors come together and place our children at a huge disadvantage, but we are committed to providing these students with a rigorous academic and therapeutic program. The school\\u2019s ethnicity is as follows: 60% Hispanic, 30% African American, 8% Caucasian, and 2% Asian.\\r\\\\n\\r\\\\nI teach a program called ACE/HOP, which stands for Academic Career Endeavors/Hands-On Program, for special needs students. In this program, my students learn the basics of Math, English, Science, and Social Studies during half the school day. For the latter part of the day, they apply the skills they have learned with hands-on vocational training. Even though they are receiving vocational training, many of my students still aspire to attend college. They are encouraged to strive for their goals, both big and small!\\r\\\\n\\r\\\\nMy students need a laptop computer in their classroom for typing reports, conducting online research, preparing presentations, and learning about computers in general.  In our technology-driven society, it is imperative that students have access to computers in order to be successful in college and their careers.\\r\\\\n\\r\\\\nThank you for considering my wonderful students!\"',\n",
        "  'cluster_length': 810})\n",
        "(4,\n",
        " {'TFIDFlist': [(u'high', 0.82274098904671555),\n",
        "                (u'poverty', 0.28969982487012208),\n",
        "                (u'area', 0.1475938290215677),\n",
        "                (u'urban', 0.10860133016068385),\n",
        "                (u'classes', 0.091848997316748587)],\n",
        "  'centroid_paragraph': u'\"I teach in an urban high need school.  My students are all special education students. \"',\n",
        "  'cluster_length': 2068})\n",
        "(5,\n",
        " {'TFIDFlist': [(u'free', 0.49891635752848595),\n",
        "                (u'lunch', 0.43344879646743101),\n",
        "                (u'reduced', 0.4081411845173662),\n",
        "                (u'receive', 0.23266538429133299),\n",
        "                (u'high', 0.203460524338446)],\n",
        "  'centroid_paragraph': u'\"I am a special education teacher. I work in a high need community: most of students receive free or reduced lunch. \"',\n",
        "  'cluster_length': 1994})\n",
        "(6,\n",
        " {'TFIDFlist': [(u'language', 0.54643597070759864),\n",
        "                (u'english', 0.51654245018618528),\n",
        "                (u'learners', 0.44586817342489815),\n",
        "                (u'high', 0.16683222114524029),\n",
        "                (u'second', 0.14688223432072764)],\n",
        "  'centroid_paragraph': u'\"I teach second grade students in a public school in a high-need community.  I have many English language learners and students with special needs in my class.   \"',\n",
        "  'cluster_length': 2087})\n",
        "(7,\n",
        " {'TFIDFlist': [(u'new', 0.6389854477234882),\n",
        "                (u'york', 0.52586256103331419),\n",
        "                (u'high', 0.30312911667822917),\n",
        "                (u'city', 0.16149002499141055),\n",
        "                (u'community', 0.12983583404625035)],\n",
        "  'centroid_paragraph': u'\"My grades 2 and 3 special education classroom is located in a high-need community in New York.  \"',\n",
        "  'cluster_length': 922})\n",
        "(8,\n",
        " {'TFIDFlist': [(u'materials', 0.16643166876205642),\n",
        "                (u'autism', 0.16643166876205642),\n",
        "                (u'preschool', 0.16204130142561873),\n",
        "                (u'kindergarten', 0.16165601112182526),\n",
        "                (u'language', 0.15258796870311059)],\n",
        "  'centroid_paragraph': u'\"I teach a special education class for children with emotional and behavior disabilities. Two of the children in my classroom have autism. I teach children in grades kindergarten, first and second grade and these children come from very unstable home lives.  The children in my classroom also have learning disabilities and speech and language disorders. \"',\n",
        "  'cluster_length': 2181})\n",
        "(9,\n",
        " {'TFIDFlist': [(u'materials', 0.33825663041384191),\n",
        "                (u'high', 0.15529431581952319),\n",
        "                (u'life', 0.15322353324706484),\n",
        "                (u'time', 0.13037440986811655),\n",
        "                (u'supplies', 0.13013490152474008)],\n",
        "  'centroid_paragraph': u'\"The special needs students in my science class need your help. \"',\n",
        "  'cluster_length': 1977})\n",
        "(10,\n",
        " {'TFIDFlist': [(u'grades', 0.83335135530684457),\n",
        "                (u'high', 0.17139310323952472),\n",
        "                (u'disabilities', 0.15499007335625836),\n",
        "                (u'12', 0.13787622971712879),\n",
        "                (u'reading', 0.099917458458095493)],\n",
        "  'centroid_paragraph': u'\"I am a special education teacher in a high school.  I teach students with special needs in grades 9-12. \"',\n",
        "  'cluster_length': 986})\n",
        "(11,\n",
        " {'TFIDFlist': [(u'self', 0.62818477454745714),\n",
        "                (u'contained', 0.62089090198377117),\n",
        "                (u'high', 0.17919974548935014),\n",
        "                (u'disabilities', 0.12718690282927472),\n",
        "                (u'autism', 0.10576115217344707)],\n",
        "  'centroid_paragraph': u'\"I have a self-contained classroom with students with special needs.  \"',\n",
        "  'cluster_length': 1327})\n",
        "(12,\n",
        " {'TFIDFlist': [(u'eligible', 0.28529210999407073),\n",
        "                (u'body', 0.25528029360491344),\n",
        "                (u'90', 0.25044505223803909),\n",
        "                (u'95', 0.24714972260332804),\n",
        "                (u'public', 0.24248541547397798)],\n",
        "  'centroid_paragraph': u'\"I work at a special education school, servicing emotionally disturbed, learning disabled, and other special education classified students. Our students come from low socioeconomic backgrounds in NYC. Over 95% of the student body is eligible for free lunch and over 90% receive public assistance. \"',\n",
        "  'cluster_length': 304})\n",
        "(13,\n",
        " {'TFIDFlist': [(u'books', 0.66199549636554877),\n",
        "                (u'reading', 0.43726223462860059),\n",
        "                (u'read', 0.35061178382699992),\n",
        "                (u'library', 0.17548023889700418),\n",
        "                (u'book', 0.14880515042085748)],\n",
        "  'centroid_paragraph': u'\"Help me obtain a variety of books for a variety of kids!\\r\\\\n\\r\\\\nI am a 4th grade special education teacher working in a high need community in New York and I am looking to fill my classroom library with books my students want to read!  My class is made up of twenty-two students and all but 1 are more than a year below grade level in reading.  Fifteen of my students have special needs. I want to find books that keep them engaged; I want to find books that interest them, at their levels.  \\r\\\\n\\r\\\\nOne of my students reads on a first grade level.  More than five read on a second grade level.  These students need books that they can read, but that aren\\'t too babyish.  They need a book made for a 10 year old, not a 6 year old.  Most 1st grade books have content that is too easy or boring; my students need concepts a bit more challenging.  My 10 year old students don\\'t want to read picture books that they call \"\"baby books.\"\"  They need lower level books with high-interest content! But I can\\'t seem to find anything like that at my school.\\r\\\\n\\r\\\\nI also have kids who love graphic novels! As long as they are invested in reading, I\\'m happy, so I\\'d love to get them more graphic novels and action stories.  By donating these books to my classroom, you are giving my students a chance to read books that are compelling to them and that will spike their interests.  \\r\\\\n\\r\\\\nEver heard of \"\"Diary of a Wimpy Kid?\"\" My students LOVE those books, but not all of them can read the text on their own.  I\\'d like to get a class set, so we can have a shared reading.  Each child can read along as we read together; we can do choral readings to help improve their fluency.  What is most important, is that they\\'ll WANT to read these books.  Your help will ensure that my students are given books that they can be excited about. \\r\\\\n\\r\\\\n\\r\\\\n\"',\n",
        "  'cluster_length': 1904})\n",
        "(14,\n",
        " {'TFIDFlist': [(u'music', 0.82726548938172317),\n",
        "                (u'instruments', 0.19310235719703175),\n",
        "                (u'musical', 0.12339366871172237),\n",
        "                (u'choir', 0.092770584021234462),\n",
        "                (u'program', 0.082034276562120659)],\n",
        "  'centroid_paragraph': u'\"Give my students the opportunity to experience music through activity.  I teach music to a self-contained special education classroom and I have no instruments to teach them with.  The need the time to play and experience music. \"',\n",
        "  'cluster_length': 643})\n",
        "(15,\n",
        " {'TFIDFlist': [(u'level', 0.36649112721060018),\n",
        "                (u'5th', 0.24332032863712932),\n",
        "                (u'first', 0.2424612275200563),\n",
        "                (u'reading', 0.19810331200572984),\n",
        "                (u'4th', 0.18816178978703052)],\n",
        "  'centroid_paragraph': u'\"I teach first grade in a high-needs school.  I teach regular education students as well as all of the special education students in the first grade. \"',\n",
        "  'cluster_length': 2189})\n",
        "(16,\n",
        " {'TFIDFlist': [(u'high', 0.6905139715723384),\n",
        "                (u'community', 0.63623776419984013),\n",
        "                (u'kindergarten', 0.076523394823634738),\n",
        "                (u'located', 0.068070229116372755),\n",
        "                (u'disabilities', 0.061841580700495509)],\n",
        "  'centroid_paragraph': u'\"I teach high school science to students with special needs, in a high-need community.\"',\n",
        "  'cluster_length': 1371})\n",
        "(17,\n",
        " {'TFIDFlist': [(u'disabilities', 0.76287957014284247),\n",
        "                (u'autism', 0.29357240601032603),\n",
        "                (u'impairments', 0.17983393183056215),\n",
        "                (u'emotional', 0.15257591402856849),\n",
        "                (u'high', 0.14904573998272161)],\n",
        "  'centroid_paragraph': u'\"I teach special education for children with severe disabilities in grades K-5.  My students have disabilities ranging from Autism to Multiple Disabilities.   \"',\n",
        "  'cluster_length': 1591})\n",
        "(18,\n",
        " {'TFIDFlist': [(u'high', 0.18922514373970689),\n",
        "                (u'program', 0.17905444921885999),\n",
        "                (u'time', 0.13020454309523855),\n",
        "                (u'new', 0.12455961684790261),\n",
        "                (u'debate', 0.12182924519798632)],\n",
        "  'centroid_paragraph': u'\"I teach in an urban high need school.  My students are all special education students. \"',\n",
        "  'cluster_length': 6863})\n",
        "(19,\n",
        " {'TFIDFlist': [(u'computer', 0.64383147257672191),\n",
        "                (u'technology', 0.22222498640092156),\n",
        "                (u'computers', 0.21091413379967705),\n",
        "                (u'laptop', 0.19736320570603499),\n",
        "                (u'access', 0.12597184857856633)],\n",
        "  'centroid_paragraph': u'\"   I work at the Queens Valley Public School in Queens. This is an elementary school where students have the opportunity to have the arts integrated into their curriculum. It is a wonderful school with a caring administration and staff.\\r\\\\n\\r\\\\nI would like to set up a computer center for my class. I have the privilege to be the special education teacher for a 5th Grade Collaborative Team Teaching Class. This class will consist of both special and regular education students. These students need to use different styles to learn the same 5th grade curriculum. \\r\\\\n\\r\\\\nI would like to maximize the use of the computer in my classroom. The computer is one of the most motivational learning tools and computer skills are essential to today\\'s job market. All students in my class would greatly benefit from this center.  \\r\\\\n\\r\\\\nWith the use of the computer, special needs students can compete with their regular education peers. The computer has valuable programs such as spell check that will enable all students to work and learn together. The computer will be used for writing workshop projects, research, educational games, etc.\\r\\\\n\\r\\\\nAlthough New York City provides materials for the computer, these are scare. I would like the students to use the computer as much as possible. We have an Epson Stylus Color 850 printer for which I would like approximately 10 black (#S020108) and 10 color (#S020089) ink cartridges. We could use approximately 10 packages of 8 x 11 computer paper mostly white and a small amount of color paper. Also we could use 3 stands that clip onto the computer to hold paper next to the monitor. \\r\\\\n\\r\\\\nThis computer center would help all students to be more effective learners and increase important skills. Please help this wonderful program that gives students that have different learning styles the same experience to succeed. \\r\\\\n\"',\n",
        "  'cluster_length': 998})\n",
        "(20,\n",
        " {'TFIDFlist': [(u'way', 0.21414328596558294),\n",
        "                (u'different', 0.16945984440583531),\n",
        "                (u'child', 0.15892129686815898),\n",
        "                (u'read', 0.15765667116363782),\n",
        "                (u'hands', 0.15554896165610255)],\n",
        "  'centroid_paragraph': u'\"My special needs students do not learn like other students learn.  They need hands on, tactile learning tools in every area of their curriculum.  My classroom currently does not have enough hands on learning tools to help support my students ability to learn. \"',\n",
        "  'cluster_length': 3446})\n",
        "(21,\n",
        " {'TFIDFlist': [(u'art', 0.8204754575156864),\n",
        "                (u'supplies', 0.12927491355003254),\n",
        "                (u'classes', 0.12318415006615004),\n",
        "                (u'high', 0.11846865142665522),\n",
        "                (u'project', 0.10912958260893829)],\n",
        "  'centroid_paragraph': u'\"I am a second year art teacher for a high needs district.  I teach pre-k through 5th grade in a school that has a high population of special needs students.  Art is a very important subject in any school, especially in high needs areas.  Art serves as an excellent outlet for children, and therefore they should receive the best art education possible. \"',\n",
        "  'cluster_length': 656})\n",
        "(22,\n",
        " {'TFIDFlist': [(u'principle', 0.35896356915009608),\n",
        "                (u'ohio', 0.34061936891552336),\n",
        "                (u'asperger', 0.29516402782362711),\n",
        "                (u'founded', 0.28956463480633776),\n",
        "                (u'art', 0.28317334222411616)],\n",
        "  'centroid_paragraph': u'\"I am the art teacher at a Northern Ohio charter school for students who have been diagnosed with ADHD and/or Asperger\\'s Syndrome (mild autism).  Our school is a special education program founded on the principle that every child can learn.  I teach 200 students at all grade levels (K-12). \"',\n",
        "  'cluster_length': 174})\n",
        "(23,\n",
        " {'TFIDFlist': [(u'inner', 0.60841588331087937),\n",
        "                (u'city', 0.60736444480678564),\n",
        "                (u'high', 0.27863626591651502),\n",
        "                (u'poverty', 0.11425075504977034),\n",
        "                (u'public', 0.089226219984502017)],\n",
        "  'centroid_paragraph': u'\"My students are special education students in an inner city. \"',\n",
        "  'cluster_length': 1075})\n",
        "(24,\n",
        " {'TFIDFlist': [(u'general', 0.73864488505907),\n",
        "                (u'inclusion', 0.28167884679281147),\n",
        "                (u'teaching', 0.14957474864606574),\n",
        "                (u'co', 0.14622672374490212),\n",
        "                (u'high', 0.13033683885235953)],\n",
        "  'centroid_paragraph': u'\"I teach fourth grade special education and general education in a high-need community.  My class is an inclusion classroom in which special education and general education students learn together. \"',\n",
        "  'cluster_length': 1285})\n",
        "(25,\n",
        " {'TFIDFlist': [(u'science', 0.8030834609826013),\n",
        "                (u'materials', 0.14065722755802176),\n",
        "                (u'hands', 0.13806047874156596),\n",
        "                (u'high', 0.10182587060723826),\n",
        "                (u'life', 0.094132144596522263)],\n",
        "  'centroid_paragraph': u'\"The special needs students in my science class need your help. \"',\n",
        "  'cluster_length': 1310})\n",
        "(26,\n",
        " {'TFIDFlist': [(u'reading', 0.846408860866811),\n",
        "                (u'read', 0.21385937803163482),\n",
        "                (u'level', 0.16110159657418674),\n",
        "                (u'books', 0.1071472468774976),\n",
        "                (u'readers', 0.097139585240208493)],\n",
        "  'centroid_paragraph': u'\"The project I hope to develop with your help is a classroom library for special education students.  The books in the library will range in difficulty from mid-second grade level to high 8th grade level. My objective is to develop a love for reading in students who do not normally enjoy reading.  To them reading is a painful and difficult experience.  \\r\\\\n\\r\\\\nI am a special education teacher at Edwards Elementary on Chicago\\'s Southwest Side. The school has 1200 students and 88 percent of the students receive free or reduced lunch. My students are sixth, seventh and eight graders who are reading at various levels.  I currently serve 15 students.  Some of them read at the second grade level and others read at the 6th grade level. They all have problems in the area of reading.  All these problem areas can become strengths if the students are given more opportunities for success in reading.  These opportunities for success are directly related to more reading materials at the students\\' independent reading level.  The students need to be able to pick up a book at their independent reading level and start reading. They also need to be able to move up to the next level when they have become fluent at their starting level.  This library will give my students opportunity to succeed and hopefully enjoy the reading experience.  The library will also remain in my class for future students to enjoy and experience reading success.  \\r\\\\n\"',\n",
        "  'cluster_length': 2120})\n",
        "(27,\n",
        " {'TFIDFlist': [(u'8th', 0.61326064193222973),\n",
        "                (u'7th', 0.5106992974624639),\n",
        "                (u'6th', 0.21425486344711345),\n",
        "                (u'high', 0.20281383124597305),\n",
        "                (u'reading', 0.13942079258195919)],\n",
        "  'centroid_paragraph': u'\"I am in a special education classroom with student that are in 6th, 7th and 8th grade. \"',\n",
        "  'cluster_length': 903})\n",
        "(28,\n",
        " {'TFIDFlist': [(u'reading', 0.15614477554344994),\n",
        "                (u'throughout', 0.15533573525566005),\n",
        "                (u'different', 0.15371765468008025),\n",
        "                (u'new', 0.14883220501939423),\n",
        "                (u'see', 0.13915492949986213)],\n",
        "  'centroid_paragraph': u'\"Our classroom is full of students all day.  We have an inclusion classroom with both regular education and special education students participating in class every day.\"',\n",
        "  'cluster_length': 1617})\n",
        "(29,\n",
        " {'TFIDFlist': [(u'technology', 0.87582617943800833),\n",
        "                (u'access', 0.13504398779780519),\n",
        "                (u'world', 0.08622861489991783),\n",
        "                (u'new', 0.066238492384572634),\n",
        "                (u'ipad', 0.065819366739162638)],\n",
        "  'centroid_paragraph': u'\"Help make learning more meaningful for my students w/ special needs through the use of technology. Technology provides access for all students, even my most challenged students can be successful w/ technology. An iPad will provide my students with an opportunity to enhance our science and math. \"',\n",
        "  'cluster_length': 1487})\n"
       ]
      }
     ],
     "prompt_number": 441
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Results:\n",
      "Found that there were meaningful clusters of need in the first paragraph with 30 clusters focused around the following categories:\n",
      "    1. Writing, time, reading, materials (4,777 similar requests)\n",
      "    2. Math, concepts, manipulatives, materials, hands (1,499 similar requests)\n",
      "    3. Low income (1,793)\n",
      "    4. English language learners (2,087)\n",
      "    5. Books, reading, read, library, book (1,904 similar requests)\n",
      "But there seemed to be too many clusters, as reading & books created two separate clusters and other seemingly similar topics were creating separate clusters, in order to account for this we looked at creating fewer clusters and experimented with MiniBatchKMeans Clustering so as to speed up the analysis (without losing accuracy)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Altered function slightly to account for differences in MiniBatchKMeans Clustering and add ability to change additional parameters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def miniclustering_function(subsetcorpus, clustermodel, n_clusters, batch_size, special_stopwords=[], normalized=False):\n",
      "    \n",
      "    clean_corpus =[]\n",
      "    for doc in subsetcorpus:\n",
      "        clean_corpus.append(doc.replace(\"\\r\\\\n\\r\\\\n\", \" \").replace('\\u201c','').replace('\\u201d',''))\n",
      "        ##cleans out spurious punctuation\n",
      "        \n",
      "    clean_set = list(set(clean_corpus))\n",
      "    ##insures no duplicate entries in corpus\n",
      "    \n",
      "    stop = stopwords.words(\"english\") \n",
      "    stop += [\".\", \",\", \"(\", \")\", \"'\", '\"', '\\\\n', \"\\n\", '\\n\"']\n",
      "    stop += special_stopwords\n",
      "    \n",
      "    vectorizer = TfidfVectorizer(stop_words=stop)\n",
      "    doc_vectors = vectorizer.fit_transform(clean_set)\n",
      "    ##returns document vectors for each document in the clean set\n",
      "    \n",
      "    if normalized:\n",
      "        doc_vectors = normalize(doc_vectors)\n",
      "        ##normalized document vectors to account for variance in length of entry. Optional parameter\n",
      "    \n",
      "    est = clustermodel(n_clusters = n_clusters, batch_size=batch_size)\n",
      "    \n",
      "    model = est.fit(doc_vectors)\n",
      "    labels = model.labels_\n",
      "    ##fits the cluster model and returns the model labels\n",
      "    \n",
      "    sorted_labels = sorted(labels)\n",
      "    \n",
      "    cluster_centers = model.cluster_centers_\n",
      "    cluster_lengths = Counter(labels)\n",
      "    \n",
      "    clustered_text = [(y,x) for (y,x) in sorted(zip(labels, clean_set))]\n",
      "    \n",
      "    list_of_ask_clusters = []\n",
      "    \n",
      "    TFIDFlist = cluster_top_TFIDF(clustered_text)\n",
      "    ##calls additional function to return the top TF-IDF words in each cluster\n",
      "    \n",
      "    for i, cluster_center in enumerate(cluster_centers):\n",
      "        \n",
      "        values = doc_vectors[i].data\n",
      "        column = doc_vectors[i].indices\n",
      "        \n",
      "        distances = pairwise_distances(doc_vectors, cluster_center, metric=\"cosine\")\n",
      "             \n",
      "        index = min(enumerate(distances), key=itemgetter(1))[0]\n",
      "    \n",
      "        cluster_details = {'cluster_length': cluster_lengths[i], \n",
      "                           'centroid_paragraph' : clean_set[index],\n",
      "                           'TFIDFlist': TFIDFlist[i]}\n",
      "        list_of_ask_clusters.append(cluster_details)\n",
      "        \n",
      "    return list_of_ask_clusters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##created additional list of stop words to clean out words that hold little meaning\n",
      "extra_stop = [\"my\", \"student\", \"students\", 'teach', 'the', 'learn', 'need', 'we',\n",
      "               'us', 'help', 'would', 'I', 'want', 'do', 'remember', 'school', 'our',\n",
      "               'n', 'nI', 'every', 'day', 'have', 'ever', 'love', 'My', 'can', 'imagine',\n",
      "               'We', 'please', 'teacher', 'it', 'It', 'This', 'year', 'many', \"'s\", 'in', 'In',\n",
      "               \"ca\", \"n't\", \"able\", \"Do\", \"like\", \"use\", \"Our\", \"could\", \"A\", \"typical\", \"around\",\n",
      "               \"'ll\", \"The\", \"What\", \"better\", \"make\", 'also', 'get', 'They', 'much','Thank', \n",
      "               'thank', 'These', 'using', 'come', 'ago', 'high']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "miniclustering_function(specialneeds_corpus, MiniBatchKMeans, 8, 10000, extra_stop, normalized=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 48,
       "text": [
        "[{'TFIDFlist': [(u'free', 0.54197762905365188),\n",
        "   (u'lunch', 0.47559947485234727),\n",
        "   (u'reduced', 0.36541173887818168),\n",
        "   (u'receive', 0.22668139659745512),\n",
        "   (u'population', 0.1188168960203352)],\n",
        "  'centroid_paragraph': u'\"I am a special education teacher. I work in a high need community: most of students receive free or reduced lunch. \"',\n",
        "  'cluster_length': 1573},\n",
        " {'TFIDFlist': [(u'disabilities', 0.26701698320657735),\n",
        "   (u'autism', 0.22855546956335013),\n",
        "   (u'kindergarten', 0.17183165447801504),\n",
        "   (u'language', 0.15993348838694477),\n",
        "   (u'preschool', 0.15882668223893823)],\n",
        "  'centroid_paragraph': u'\"I teach a special education class for children with emotional and behavior disabilities. Two of the children in my classroom have autism. I teach children in grades kindergarten, first and second grade and these children come from very unstable home lives.  The children in my classroom also have learning disabilities and speech and language disorders. \"',\n",
        "  'cluster_length': 2660},\n",
        " {'TFIDFlist': [(u'reading', 0.67087470036381824),\n",
        "   (u'books', 0.41943004202562384),\n",
        "   (u'read', 0.32226604872522496),\n",
        "   (u'level', 0.15616814125900258),\n",
        "   (u'book', 0.11850068101839187)],\n",
        "  'centroid_paragraph': u'\"I would like to get reading books for my 6th, 7th and 8th grade Special Education class.  I want to get books that are at their interest level, but at a lower reading level.   My students have a low reading level, and most books in their area are made for younger children.  I feel chapter books for our Reading Fluency class would be great.  During this class we read together, and in pairs. I also want books that will help them with their reading comprehension.  This will also help when they read their reading books.  Children in Special Education have a difficult time reading, and if they don\\'t have anything that will interest them, they won\\'t want to read.  I want them to learn to like reading.\\r\\\\n\"',\n",
        "  'cluster_length': 3043},\n",
        " {'TFIDFlist': [(u'community', 0.27441211651933661),\n",
        "   (u'level', 0.20673436909260232),\n",
        "   (u'general', 0.19378042524920397),\n",
        "   (u'inclusion', 0.1863781716244049),\n",
        "   (u'first', 0.17527479118720632)],\n",
        "  'centroid_paragraph': u'\"I teach fourth grade special education and general education in a high-need community.  My class is an inclusion classroom in which special education and general education students learn together. \"',\n",
        "  'cluster_length': 4422},\n",
        " {'TFIDFlist': [(u'language', 0.52349538239745064),\n",
        "   (u'english', 0.45521337599778311),\n",
        "   (u'learners', 0.33389680152070406),\n",
        "   (u'low', 0.24484292262405036),\n",
        "   (u'income', 0.20970752127276515)],\n",
        "  'centroid_paragraph': u'\"I teach 5th grade in a Title I school where I have a high population of English Language Learners, special needs students and low income families. \"',\n",
        "  'cluster_length': 2854},\n",
        " {'TFIDFlist': [(u'math', 0.36722532041179479),\n",
        "   (u'materials', 0.2854191884281681),\n",
        "   (u'art', 0.22015473754417184),\n",
        "   (u'activities', 0.15654445477013856),\n",
        "   (u'hands', 0.14225845745682142)],\n",
        "  'centroid_paragraph': u'\"My students have special needs, so they learn best when given hands-on projects. By integrating art into our class and participating in an art class, my students can work on fine motor skills, academic skills, and social skills by using art in the classroom. With no funding, we have few materials! \"',\n",
        "  'cluster_length': 3161},\n",
        " {'TFIDFlist': [(u'time', 0.18630183093001837),\n",
        "   (u'new', 0.17073066426663211),\n",
        "   (u'disabilities', 0.11275291605189612),\n",
        "   (u'program', 0.11087554134779991),\n",
        "   (u'writing', 0.11032337231718337)],\n",
        "  'centroid_paragraph': u'\"Special Education teacher needs to create a small area in a general education classroom to work with those students who need the most help. I am a special education teacher in a collaborative team teaching classroom in a high-need urban community. I teach 22 second graders of which seven have special needs. \"',\n",
        "  'cluster_length': 9446},\n",
        " {'TFIDFlist': [(u'technology', 0.41631652112926509),\n",
        "   (u'science', 0.23005113139168723),\n",
        "   (u'computer', 0.21231157046429888),\n",
        "   (u'program', 0.17739560927388368),\n",
        "   (u'new', 0.12811905114224931)],\n",
        "  'centroid_paragraph': u'\"I work at a special education school, servicing emotionally disturbed, learning disabled, and other special education classified students. Our students come from low socioeconomic backgrounds. Over 95% of the student body is eligible for free lunch and over 90% receive public assistance. These factors come together and place our children at a huge disadvantage, but we are committed to providing these students with a rigorous academic and therapeutic program. The school\\u2019s ethnicity is as follows: 60% Hispanic, 30% African American, 8% Caucasian, and 2% Asian. I have a self-contained classroom, which services students with specialized learning needs. During classroom lessons, my students learn the basics of Math, English, Science, and Social Studies. They only venture out of my classroom for their elective courses, so the majority of their studies are done in my room alone. Despite my students\\' various emotional, environmental, and educational challenges, many of my students plan to attend college or trade school when they graduate. They realize they need to be prepared for our technology-driven society, and are eager to utilize as much equipment as possible. I would love to have an Interactive Markerboard in the classroom, which my students would use on a daily basis. This technology is compatible with our classroom computer & LCD projector. My students would use all of these resources to become more actively engaged in classroom learning. They would practice math concepts with digitally projected images; give presentations and \"\"draw\"\" on their slideshows; view video clips related to subject material; visually explore the earth & the solar system; and delve into a more collective exploration of their learning. My kids are such visual learners, and I love to provide them with unique learning experiences as much as possible. Thank you for considering my deserving students. They would love to have this innovative technology right in their own classroom!\"',\n",
        "  'cluster_length': 2991}]"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Results:\n",
      "Looking at the new, smaller clusters (now only using 8 as seemed to continue to have duplicate clusters with 10) we can pull out more interpretable groups of asks:\n",
      "\n",
      "1 ASK: Focused on a new program for kids with disabilities (not one of the more well-defined asks)\n",
      "        Key Words - time, new, disabilities, program, writing\n",
      "        Number of asks - 9,446\n",
      "        This seems to pull in a lot of asks and isn't a well-defined category\n",
      "\n",
      "2 ASK: Focused on inclusion classrooms (classrooms with special education and general education students together)\n",
      "        Key Words - community, level, inclusion\n",
      "        Number of asks - 4,422\n",
      "        \n",
      "3 ASK: Seems to have combined any category where kids would be learning with their hands or additional activities, which means that Math and Art requests were clustered together. With additional clusters, these two categories formed different clusters.\n",
      "        Key words - math, art, activities, hands\n",
      "        Number of asks - 3,161\n",
      "\n",
      "4 ASK: This cluster is clearly focused on reading materials and books for classrooms\n",
      "        Key words - reading, books, read\n",
      "        Number of asks - 3,043\n",
      "\n",
      "5 ASK: This cluster is focused on technology, science and computers in classrooms\n",
      "        Key words - technology, science, computers\n",
      "        Number of asks - 2,991\n",
      "        \n",
      "6 ASK: English Language Learners are a large category of need within the dataset, focused on children who don't speak English as their first language\n",
      "        Key words - english, language, learners\n",
      "        Number of asks - 2,854\n",
      "\n",
      "7 ASK: Seems to be focused on younger children with autism or other specific disabilities\n",
      "        Key words - disabilities, autism, kindergarten, preschool\n",
      "        Number of asks - 2,660\n",
      "\n",
      "8 ASK: Clustered by the mention of \"Free Lunch\" which is an indicator of high need communities where many of the students are provided lunch through government subsidies\n",
      "        Key words - free, lunch\n",
      "        Number of asks - 1,573\n",
      "\n",
      "One of the drawbacks of fewer clusters is that while we have fewer similar-seeming groups, we also miss some of the more nuanced groups with had with 20 clusters, such as Music and Art as specific categories of asks."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}